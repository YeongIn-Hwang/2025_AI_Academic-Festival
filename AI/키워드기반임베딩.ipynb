{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"all_places.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_places = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43a2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_review(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = text.strip()\n",
    "\n",
    "    if len(text) < 3:\n",
    "        return None\n",
    "\n",
    "    if not re.search(\"[ê°€-í£]\", text):  # í•œê¸€ ì—†ëŠ” ì™¸êµ­ì–´ ë¦¬ë·° ì œê±°\n",
    "        return None\n",
    "\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # ë°˜ë³µ ë¬¸ì ì •ë¦¬\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£.,!?]\", \"\", text)  # ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = text[:300]  # ë„ˆë¬´ ê¸´ ë¦¬ë·° ìë¥´ê¸°\n",
    "\n",
    "    return text if text else None\n",
    "\n",
    "for place in all_places:\n",
    "    original_reviews = place.get(\"reviews\", [])\n",
    "    cleaned_reviews = []\n",
    "\n",
    "    for review in original_reviews:\n",
    "        cleaned = clean_review(review)\n",
    "        if cleaned:\n",
    "            cleaned_reviews.append(cleaned)\n",
    "\n",
    "    place[\"reviews\"] = cleaned_reviews  # âœ… ì „ì²˜ë¦¬ëœ ë¦¬ë·°ë¡œ ë®ì–´ì“°ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬í–‰ì—ì„œì˜ í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œ : ì¢…ë£Œ\n",
      "ì—¬í–‰ì—ì„œì˜ ë¹„í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œ : ì¢…ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ì—¬í–‰ì—ì„œì˜ í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì˜ˆ : ì „ë§ëŒ€ ê³µì› ìì—°\")\n",
    "\"\"\"keyword_hope = []\n",
    "for i in range(10):\n",
    "    keyword = input()\n",
    "    if (keyword == \"ì¢…ë£Œ\"):\n",
    "        break\n",
    "    keyword_hope.append(keyword)\"\"\"\n",
    "\n",
    "print(\"ì—¬í–‰ì—ì„œì˜ ë¹„í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì˜ˆ : ì‹œë„ëŸ¬ì›€ í˜¼ì¡\")\n",
    "\"\"\"keyword_nonhope = []\n",
    "for i in range(10):\n",
    "    keyword = input()\n",
    "    if (keyword == \"ì¢…ë£Œ\"):\n",
    "        break\n",
    "    keyword_nonhope.append(keyword)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fac61e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (í•œ ì¤„ë¡œ ë)\n",
    "model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_sbert_embedding(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "    return model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "def get_sbert_review_vector(reviews):\n",
    "    embeddings = [\n",
    "        get_sbert_embedding(review)\n",
    "        for review in reviews if review.strip()\n",
    "    ]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "def get_place_vector_with_name(place, review_weight=1.0, name_weight=0):\n",
    "    reviews = place.get(\"reviews\", [])\n",
    "    name = place.get(\"name\", \"\")\n",
    "\n",
    "    review_vec = get_sbert_review_vector(reviews)\n",
    "    name_vec = get_sbert_embedding(name)\n",
    "\n",
    "    total_weight = review_weight + name_weight\n",
    "    final_vec = (review_weight * review_vec + name_weight * name_vec) / total_weight\n",
    "    return final_vec\n",
    "\n",
    "# ğŸ” ë¬¸ë§¥ ë²¡í„° ìƒì„± ë° ì €ì¥\n",
    "for place in all_places:\n",
    "    if \"reviews\" in place and \"name\" in place:\n",
    "        place[\"review_vector\"] = get_place_vector_with_name(place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff39fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_hope = [\"ë£¨í”„íƒ‘\",\"ì•¼ê²½\",\"ì „ë§ëŒ€\",\"ê³ ì¸µê±´ë¬¼\",\"íŠ¸ëœë“œ ì¹´í˜\",\"íŒŒí‹°\",\"ë°”\",\"ì£¼ë¥˜\",\"ê³µì›\",\"í•œê°• ë·°\"]\n",
    "keyword_nonhope = [\"ë²Œë ˆ\",\"ë¨¼ì§€\",\"í˜¼ì¡\",\"ë”ëŸ¬ì›€\"]\n",
    "\n",
    "def clean_keyword(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = text.strip()\n",
    "\n",
    "    if not re.search(\"[ê°€-í£]\", text):  # í•œê¸€ ì—†ëŠ” ì™¸êµ­ì–´ ë¦¬ë·° ì œê±°\n",
    "        return None\n",
    "\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # ë°˜ë³µ ë¬¸ì ì •ë¦¬\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£.,!?]\", \"\", text)  # ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = text[:300]  # ë„ˆë¬´ ê¸´ ë¦¬ë·° ìë¥´ê¸°\n",
    "\n",
    "    return text if text else None\n",
    "\n",
    "keyword_hope = [clean_keyword(r) for r in keyword_hope if r]\n",
    "keyword_nonhope = [clean_keyword(r) for r in keyword_nonhope if r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "018252c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['íŠ¸ëœë“œ ì¹´í˜', 'íŒŒí‹°', 'ë°”', 'ì£¼ë¥˜'], ['ì•¼ê²½', 'ì „ë§ëŒ€', 'ê³µì›', 'í•œê°• ë·°'], ['ë£¨í”„íƒ‘', 'ê³ ì¸µê±´ë¬¼']]\n",
      "['ë²Œë ˆ', 'ë¨¼ì§€', 'í˜¼ì¡', 'ë”ëŸ¬ì›€']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# 1. í¬ë§ í‚¤ì›Œë“œ ì„ë² ë”©\n",
    "hope_embeddings = [get_sbert_embedding(k) for k in keyword_hope if k]\n",
    "\n",
    "# 2. ìµœì  K íƒìƒ‰\n",
    "best_k = 2\n",
    "best_score = -1\n",
    "for k in range(2, min(len(hope_embeddings), 6)):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\").fit(hope_embeddings)\n",
    "    score = silhouette_score(hope_embeddings, kmeans.labels_)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "\n",
    "# 3. ìµœì  Kë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰\n",
    "final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=\"auto\").fit(hope_embeddings)\n",
    "labels = final_kmeans.labels_\n",
    "\n",
    "clustered_keywords_list = [[] for _ in range(best_k)]\n",
    "cluster_centroids = []\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° IDë³„ í‚¤ì›Œë“œì™€ ë²¡í„° ìˆ˜ì§‘\n",
    "cluster_vectors = [[] for _ in range(best_k)]\n",
    "\n",
    "for keyword, label in zip(keyword_hope, labels):\n",
    "    clustered_keywords_list[label].append(keyword)\n",
    "    cluster_vectors[label].append(get_sbert_embedding(keyword))  # ê° í‚¤ì›Œë“œì˜ ë²¡í„° ì €ì¥\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ë²¡í„° ê³„ì‚°\n",
    "for vectors in cluster_vectors:\n",
    "    if vectors:\n",
    "        centroid = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        centroid = np.zeros(model.get_sentence_embedding_dimension())\n",
    "    cluster_centroids.append(centroid)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "keyword_hope = clustered_keywords_list  # [[cluster1 í‚¤ì›Œë“œë“¤], [cluster2 í‚¤ì›Œë“œë“¤], ...]\n",
    "keyword_hope_centroids = cluster_centroids  # [cluster1 ë²¡í„°, cluster2 ë²¡í„°, ...]\n",
    "\n",
    "print(keyword_hope)\n",
    "print(keyword_nonhope)\n",
    "\n",
    "# ë¹„ì„ í˜¸ í‚¤ì›Œë“œ ì„ë² ë”©\n",
    "nonhope_embeddings = [get_sbert_embedding(k) for k in keyword_nonhope if k]\n",
    "\n",
    "# í‰ê·  ì„ë² ë”© (ë¹ˆ ê²½ìš° ëŒ€ë¹„)\n",
    "if nonhope_embeddings:\n",
    "    nonhope_mean_vector = np.mean(nonhope_embeddings, axis=0)\n",
    "else:\n",
    "    nonhope_mean_vector = np.zeros(model.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31421b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cluster_scores(review_vector, name_vector, cluster_centroids, alpha=0.2):\n",
    "    scores = []\n",
    "    for centroid in cluster_centroids:\n",
    "        # ê° ìœ ì‚¬ë„ ê°œë³„ ê³„ì‚°\n",
    "        sim_review = (\n",
    "            cosine_similarity([review_vector], [centroid])[0][0]\n",
    "            if np.linalg.norm(review_vector) > 0 and np.linalg.norm(centroid) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        sim_name = (\n",
    "            cosine_similarity([name_vector], [centroid])[0][0]\n",
    "            if np.linalg.norm(name_vector) > 0 and np.linalg.norm(centroid) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        # ì´ë¦„ê³¼ ë¦¬ë·°ì˜ ê°€ì¤‘í•©\n",
    "        score = (1 - alpha) * sim_review + alpha * sim_name\n",
    "        scores.append(round(score, 4))\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ëª¨ë“  ì¥ì†Œì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ë¶€ì—¬\n",
    "for place in all_places:\n",
    "    review_vec = place.get(\"review_vector\")\n",
    "    name_vec = get_sbert_embedding(place.get(\"name\", \"\"))\n",
    "    if review_vec is not None:\n",
    "        place[\"cluster_scores\"] = compute_cluster_scores(\n",
    "            review_vec, name_vec, keyword_hope_centroids, alpha=0.2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edae500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Type: tourist_attraction\n",
      "  - í‘ì„ë™ê³µì› | ì ìˆ˜: [0.24, 0.61, 0.23]\n",
      "  - ì‚°ê¸°ìŠ­ê³µì› | ì ìˆ˜: [0.13, 0.55, 0.22]\n",
      "  - ì„œë˜ì„¬ | ì ìˆ˜: [0.19, 0.54, 0.18]\n",
      "  - ë§ˆì„ìˆ²ê³µì› | ì ìˆ˜: [0.21, 0.53, 0.18]\n",
      "  - ì›íš¨ëŒ€êµ | ì ìˆ˜: [0.22, 0.52, 0.35]\n",
      "  - ê°€ë¡œê³µì› | ì ìˆ˜: [0.28, 0.52, 0.29]\n",
      "  - ë•ìˆ˜ê³µì› | ì ìˆ˜: [0.20, 0.48, 0.23]\n",
      "  - ê±°ë¦¬ê³µì› | ì ìˆ˜: [0.12, 0.48, 0.12]\n",
      "  - ê³„ë‚¨ì œ1ê·¼ë¦°ê³µì› | ì ìˆ˜: [0.13, 0.48, 0.15]\n",
      "  - ê´€ì•…ì‚° ìì—°ê³µì› | ì ìˆ˜: [0.12, 0.46, 0.09]\n",
      "\n",
      "ğŸ“‚ Type: cafe\n",
      "  - ë¡œì–„í˜¸ìŠ¤íŠ¸ | ì ìˆ˜: [0.49, 0.42, 0.32]\n",
      "  - ìŠ¤ë¬´ë””í‚¹ ì˜ë“±í¬íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.44, 0.24, 0.16]\n",
      "  - ë™ì‘ë…¸ì„ì¹´í˜ | ì ìˆ˜: [0.26, 0.43, 0.25]\n",
      "  - íˆ¬ì¸í”Œë ˆì´ìŠ¤ ì„œê°•ëŒ€ì  | ì ìˆ˜: [0.41, 0.33, 0.19]\n",
      "  - í• ë¦¬ìŠ¤ ì»¤í”¼ êµ¬ë¡œì  | ì ìˆ˜: [0.40, 0.25, 0.21]\n",
      "  - íˆ¬ì¸í”Œë ˆì´ìŠ¤ ê°€ì‚°ë””ì§€í„¸ì  | ì ìˆ˜: [0.37, 0.24, 0.14]\n",
      "  - ì‹ ì´Œë¯¸í”Œ | ì ìˆ˜: [0.36, 0.30, 0.14]\n",
      "  - bar sting | ì ìˆ˜: [0.35, 0.27, 0.17]\n",
      "  - ë¹„í•˜ì¸ë“œ | ì ìˆ˜: [0.35, 0.23, 0.11]\n",
      "  - ì–´ë°˜íŠ¸ë¦¬ | ì ìˆ˜: [0.34, 0.35, 0.22]\n",
      "\n",
      "ğŸ“‚ Type: bar\n",
      "  - ì¹´ìŠ¤íƒ€ìš´ | ì ìˆ˜: [0.46, 0.29, 0.18]\n",
      "  - ì™€ë°” ì—¬ì˜ë„ ì§ì˜ì  | ì ìˆ˜: [0.43, 0.31, 0.18]\n",
      "  - ìƒí™œë§¥ì£¼ êµ¬ë¡œë””ì§€í„¸ì  | ì ìˆ˜: [0.42, 0.26, 0.16]\n",
      "  - ì¹˜ì–´ìŠ¤ì˜ë“±í¬êµ¬ì²­ì  | ì ìˆ˜: [0.42, 0.35, 0.21]\n",
      "  - í†µíŒŒì´ë¸Œ ë‹¹ì‚°ì  | ì ìˆ˜: [0.39, 0.25, 0.18]\n",
      "  - í‘¸ë¥¸ìœ ì›” | ì ìˆ˜: [0.39, 0.34, 0.13]\n",
      "  - ì§íƒœ&ë…¸ê°€ë¦¬ | ì ìˆ˜: [0.39, 0.22, 0.16]\n",
      "  - ìƒí™œë§¥ì£¼ ê°€ì‚°ë””ì§€í„¸ì  | ì ìˆ˜: [0.39, 0.26, 0.15]\n",
      "  - ë§ˆë¥´ì…€ Marcel | ì ìˆ˜: [0.39, 0.19, 0.17]\n",
      "  - í›Œë„ë¼ìˆ¯ë¶ˆë°”ë² í | ì ìˆ˜: [0.38, 0.23, 0.20]\n",
      "\n",
      "ğŸ“‚ Type: bakery\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ì—¬ì˜ë„ì  | ì ìˆ˜: [0.41, 0.32, 0.21]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ êµ¬ë¡œì—˜ë¦¼ì  | ì ìˆ˜: [0.39, 0.29, 0.25]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ê°€ì‚°ì—ì´ìŠ¤ì  | ì ìˆ˜: [0.38, 0.21, 0.22]\n",
      "  - ë˜í‚¨ë„ë„ˆì¸  ì—¼ì°½ì  | ì ìˆ˜: [0.37, 0.13, 0.21]\n",
      "  - ë˜í‚¨ í™ëŒ€ì—­ì  | ì ìˆ˜: [0.36, 0.25, 0.16]\n",
      "  - íŒŒë¦¬í¬ë¼ìƒ ì´ì´Œì  | ì ìˆ˜: [0.36, 0.24, 0.19]\n",
      "  - íŒŒë¦¬í¬ë¼ìƒ ì—¬ì˜ë„2í˜¸ì  | ì ìˆ˜: [0.34, 0.18, 0.16]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ì—¼ì°½ì  | ì ìˆ˜: [0.34, 0.17, 0.17]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ë‚œê³¡ì‚¬ê±°ë¦¬ì  | ì ìˆ˜: [0.33, 0.22, 0.16]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ì˜ë“±í¬ëŒ€ìš°ì  | ì ìˆ˜: [0.33, 0.24, 0.15]\n",
      "\n",
      "ğŸ“‚ Type: restaurant\n",
      "  - ìŠ¤ë¬´ë””í‚¹ ì˜ë“±í¬íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.44, 0.24, 0.16]\n",
      "  - ë³¸ì£½ ì˜ë“±í¬ì‹œì¥ë¡œí„°ë¦¬ì  | ì ìˆ˜: [0.38, 0.23, 0.22]\n",
      "  - í”¼ìë‚˜ë¼ì¹˜í‚¨ê³µì£¼ êµ¬ë¡œ1í˜¸ì  | ì ìˆ˜: [0.37, 0.22, 0.17]\n",
      "  - ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤ êµ¬ë¡œê³ ì²™ | ì ìˆ˜: [0.35, 0.32, 0.23]\n",
      "  - ì¹˜í‚¨ë§¤ë‹ˆì•„ ì˜ë“±í¬ì—­ì  | ì ìˆ˜: [0.34, 0.27, 0.14]\n",
      "  - ì‚¿ë½€ë¡œ ëª©ë™ì  | ì ìˆ˜: [0.32, 0.28, 0.12]\n",
      "  - í•˜ë‚˜ìŠ¤ì‹œ íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.31, 0.13, 0.13]\n",
      "  - ì´ë ˆ | ì ìˆ˜: [0.29, 0.20, 0.11]\n",
      "  - í•œì¼ê´€ íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.28, 0.20, 0.16]\n",
      "  - í”¼ììŠ¤ì¿¨ êµ¬ë¡œ1ì  | ì ìˆ˜: [0.28, 0.18, 0.09]\n",
      "\n",
      "ğŸ“‚ Type: shopping_mall\n",
      "  - ë§¥ìŠ¤ì—”ì˜ | ì ìˆ˜: [0.49, 0.34, 0.28]\n",
      "  - ë”ìŠˆíŠ¸í•˜ìš°ìŠ¤ ì˜ë“±í¬í™ˆí”ŒëŸ¬ìŠ¤ | ì ìˆ˜: [0.34, 0.28, 0.16]\n",
      "  - TOUCH | ì ìˆ˜: [0.34, 0.31, 0.16]\n",
      "  - LUCE | ì ìˆ˜: [0.33, 0.24, 0.21]\n",
      "  - ì§€ì˜¤ì§€ì•„ êµ¬ë¡œì  | ì ìˆ˜: [0.33, 0.22, 0.23]\n",
      "  - YA | ì ìˆ˜: [0.29, 0.15, 0.09]\n",
      "  - ì„¸ì • íƒ€ì„ìŠ¤í€˜ì–´ì§€ì  | ì ìˆ˜: [0.19, 0.26, 0.17]\n",
      "  - ê¸ˆí™”ì£¼ë‹¨ | ì ìˆ˜: [0.07, 0.05, 0.07]\n",
      "  - ë² ë² ì•™ìŠˆ | ì ìˆ˜: [0.06, 0.02, 0.06]\n",
      "  - ì¿ ì•„ ì‹ ì„¸ê³„ì˜ë“±í¬ì§€ìƒA | ì ìˆ˜: [0.06, 0.06, 0.06]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# íƒ€ì…ë³„ ê·¸ë£¹í•‘\n",
    "type_grouped = defaultdict(list)\n",
    "for place in all_places:\n",
    "    if \"cluster_scores\" in place:\n",
    "        type_grouped[place[\"type\"]].append(place)\n",
    "\n",
    "# ì •ë ¬ ë° ì¶œë ¥\n",
    "for place_type, places in type_grouped.items():\n",
    "    print(f\"\\nğŸ“‚ Type: {place_type}\")\n",
    "    sorted_places = sorted(places, key=lambda x: max(x[\"cluster_scores\"]), reverse=True)\n",
    "    for p in sorted_places[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥\n",
    "        scores_str = \", \".join(f\"{s:.2f}\" for s in p[\"cluster_scores\"])\n",
    "        print(f\"  - {p['name']} | ì ìˆ˜: [{scores_str}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ecce99a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¹„ì„ í˜¸ í‚¤ì›Œë“œ ìœ ì‚¬ë„ (íƒ€ì…ë³„ ìƒìœ„ 3ê°œì”©):\n",
      "\n",
      "ğŸ”¹ Tourist_Attraction:\n",
      "1. ê°€ë¡œê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3953 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 49\n",
      "2. ë§¤í™”ê·¼ë¦°ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3765 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 29\n",
      "3. ê¸ˆì²œêµ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3528 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 169\n",
      "4. ë°©í™”ìŒˆì§€ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3411 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 167\n",
      "5. ì„œë˜ì„¬\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3358 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 762\n",
      "6. í‘ì„ë™ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3261 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 97\n",
      "7. ë•ìˆ˜ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3092 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 51\n",
      "8. ì´í™”ì—¬ìëŒ€í•™êµ ìì—°ì‚¬ë°•ë¬¼ê´€\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3009 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 111\n",
      "9. ëˆì˜ë¬¸(ì„œëŒ€ë¬¸) í„°\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2893 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 51\n",
      "10. ë¶€ì²œì›ë¯¸ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2827 | í‰ì : 4.3 | ë¦¬ë·° ìˆ˜: 289\n",
      "\n",
      "ğŸ”¹ Cafe:\n",
      "1. íƒì•¤íƒìŠ¤ ì˜ë“±í¬ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4440 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 130\n",
      "2. ì‹ ì´Œë¯¸í”Œ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4241 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 91\n",
      "3. ì–´ë°˜íŠ¸ë¦¬\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4031 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 1\n",
      "4. ì»¤í”¼ë¹ˆ í™ëŒ€ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3678 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 556\n",
      "5. ì•„ë¦¬ìŠ¤íƒ€ì»¤í”¼ ë“±ì´Œì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3416 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 14\n",
      "6. ë¹„í•˜ì¸ë“œ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3274 | í‰ì : 4.3 | ë¦¬ë·° ìˆ˜: 63\n",
      "7. ì´ë””ì•¼ì»¤í”¼ ì„œì—¬ì˜ë„ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3260 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 116\n",
      "8. í• ë¦¬ìŠ¤ ì»¤í”¼ êµ¬ë¡œì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3207 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 88\n",
      "9. í•¸ë“œí”½íŠ¸í˜¸í…”\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3183 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 599\n",
      "10. íŒŒìŠ¤ì¿ ì°Œë³´ë¼ë§¤ê³µì›ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3131 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 203\n",
      "\n",
      "ğŸ”¹ Bar:\n",
      "1. ë¹„ë‹í•˜ìš°ìŠ¤\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.5079 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 148\n",
      "2. í‘¸ë¥¸ìœ ì›”\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3965 | í‰ì : 4.2 | ë¦¬ë·° ìˆ˜: 5\n",
      "3. ì¹˜ì¹˜ í™ëŒ€ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3845 | í‰ì : 4.2 | ë¦¬ë·° ìˆ˜: 81\n",
      "4. ì´í™”ì£¼ë§‰\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3712 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 2\n",
      "5. MAG Live Club\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3710 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 3\n",
      "6. ì§íƒœ&ë…¸ê°€ë¦¬\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3609 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 15\n",
      "7. ì™€ë¼ì™€ë¼\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3600 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 114\n",
      "8. ì™€ë°” ì˜ë“±í¬ ì§ì˜ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3549 | í‰ì : 4.5 | ë¦¬ë·° ìˆ˜: 2\n",
      "9. ë˜ë˜ì˜¤ë˜\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3232 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 4\n",
      "10. íˆ¬ë‹¤ë¦¬ êµ¬ë¡œì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3213 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 22\n",
      "\n",
      "ğŸ”¹ Bakery:\n",
      "1. ë˜í‚¨ í™ëŒ€ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3713 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 158\n",
      "2. íŒŒë¦¬í¬ë¼ìƒ ì—¬ì˜ë„2í˜¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3304 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 309\n",
      "3. íŒŒë¦¬ë°”ê²Œëœ¨ ì˜¤ëª©êµì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3220 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 13\n",
      "4. íŒŒë¦¬ë°”ê²Œëœ¨ ì—¬ì˜ë„ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3211 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 25\n",
      "5. ë¹µêµ¼í„° ëŒ€ë¦¼ë™ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3209 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 1\n",
      "6. íŒŒë¦¬ë°”ê²Œëœ¨ êµ¬ë¡œë””ì§€í„¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3085 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 14\n",
      "7. ë˜í‚¨ë„ë„ˆì¸  ì—¼ì°½ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3079 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 23\n",
      "8. íŒŒë¦¬ë°”ê²Œëœ¨ ê¸ˆì²œêµ¬ì²­ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3008 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 15\n",
      "9. ë˜í‚¨ë„ë„ˆì¸  êµ¬ë¡œíƒœí‰ì–‘ë¬¼ì‚°ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2919 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 59\n",
      "10. íŒŒë¦¬ë°”ê²Œëœ¨ ê°€ì‚°ì—ì´ìŠ¤ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2918 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 9\n",
      "\n",
      "ğŸ”¹ Restaurant:\n",
      "1. ë³¸ì£½ ì˜ë“±í¬ì‹œì¥ë¡œí„°ë¦¬ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3665 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 11\n",
      "2. ì´ë ˆ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3315 | í‰ì : 4.8 | ë¦¬ë·° ìˆ˜: 12\n",
      "3. ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤ êµ¬ë¡œê³ ì²™\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3303 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 50\n",
      "4. í•˜ë‚˜ìŠ¤ì‹œ íƒ€ì„ìŠ¤í€˜ì–´ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3211 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 4\n",
      "5. í•¸ë“œí”½íŠ¸í˜¸í…”\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3183 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 599\n",
      "6. ë¡¯ë°ë¦¬ì•„ êµ¬ë¡œì‹œì¥ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3129 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 321\n",
      "7. í•œì¼ê´€ íƒ€ì„ìŠ¤í€˜ì–´ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2942 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 409\n",
      "8. í”¼ìë‚˜ë¼ì¹˜í‚¨ê³µì£¼ êµ¬ë¡œ1í˜¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2681 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 15\n",
      "9. í”¼ììŠ¤ì¿¨ ì˜ë“±í¬ë‚¨ë¶€ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2624 | í‰ì : 4.5 | ë¦¬ë·° ìˆ˜: 72\n",
      "10. ìŠ¤ë¬´ë””í‚¹ ì˜ë“±í¬íƒ€ì„ìŠ¤í€˜ì–´ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2613 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 10\n",
      "\n",
      "ğŸ”¹ Shopping_Mall:\n",
      "1. ì§€ì˜¤ì§€ì•„ êµ¬ë¡œì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3878 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 7\n",
      "2. LUCE\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3819 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 6\n",
      "3. TOUCH\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2929 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 5\n",
      "4. í† íŠ¸ë¼\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2928 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 1\n",
      "5. YA\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2805 | í‰ì : 5 | ë¦¬ë·° ìˆ˜: 1\n",
      "6. ë§¥ìŠ¤ì—”ì˜\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2804 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 13\n",
      "7. ì„¸ì • íƒ€ì„ìŠ¤í€˜ì–´ì§€ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2637 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 3\n",
      "8. ê¸ˆí™”ì£¼ë‹¨\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2628 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 4\n",
      "9. ë”ìŠˆíŠ¸í•˜ìš°ìŠ¤ ì˜ë“±í¬í™ˆí”ŒëŸ¬ìŠ¤\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2534 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 27\n",
      "10. ì²œì‚¬ë“¤ì˜í•©ì°½\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2291 | í‰ì : 4.3 | ë¦¬ë·° ìˆ˜: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# ì´ë¦„ê³¼ ë¦¬ë·°ë¥¼ í•¨ê»˜ ê³ ë ¤í•œ ë²¡í„° (ê°€ì¤‘ì¹˜ ì¡°ì ˆ ê°€ëŠ¥)\n",
    "def get_combined_place_vector(place, review_weight=1.0, name_weight=1.0):\n",
    "    review_vec = place.get(\"review_vector\", np.zeros(model.get_sentence_embedding_dimension()))\n",
    "    name_vec = get_sbert_embedding(place.get(\"name\", \"\"))\n",
    "    \n",
    "    if np.linalg.norm(review_vec) == 0 and np.linalg.norm(name_vec) == 0:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    total_weight = review_weight + name_weight\n",
    "    return (review_weight * review_vec + name_weight * name_vec) / total_weight\n",
    "\n",
    "# ì¥ì†Œë³„ ë¹„ì„ í˜¸ ìœ ì‚¬ë„ ê³„ì‚° (ì´ë¦„ í¬í•¨)\n",
    "for place in all_places:\n",
    "    combined_vec = get_combined_place_vector(place, review_weight=1.0, name_weight=1.0)\n",
    "    if np.linalg.norm(combined_vec) > 0 and np.linalg.norm(nonhope_mean_vector) > 0:\n",
    "        score = cosine_similarity([combined_vec], [nonhope_mean_vector])[0][0]\n",
    "        place[\"nonhope_score\"] = round(score, 4)\n",
    "    else:\n",
    "        place[\"nonhope_score\"] = 0.0\n",
    "\n",
    "# íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™” ë° ì¶œë ¥\n",
    "type_to_places = defaultdict(list)\n",
    "for place in all_places:\n",
    "    type_to_places[place[\"type\"]].append(place)\n",
    "\n",
    "print(\"\\në¹„ì„ í˜¸ í‚¤ì›Œë“œ ìœ ì‚¬ë„ (íƒ€ì…ë³„ ìƒìœ„ 3ê°œì”©):\")\n",
    "for place_type, places in type_to_places.items():\n",
    "    print(f\"\\nğŸ”¹ {place_type.title()}:\")\n",
    "    top_places = sorted(places, key=lambda x: x[\"nonhope_score\"], reverse=True)[:10]\n",
    "    for i, place in enumerate(top_places, 1):\n",
    "        print(f\"{i}. {place['name']}\")\n",
    "        print(f\"ë¹„ì„ í˜¸ ìœ ì‚¬ë„: {place['nonhope_score']:.4f} | í‰ì : {place['rating']} | ë¦¬ë·° ìˆ˜: {place['user_ratings_total']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43834683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_place_for_json(place):\n",
    "    p = place.copy()\n",
    "    for key in [\"review_vector\", \"name_vector\"]:\n",
    "        if isinstance(p.get(key), np.ndarray):\n",
    "            p[key] = p[key].tolist()\n",
    "    if \"cluster_scores\" in p:\n",
    "        p[\"cluster_scores\"] = list(map(float, p[\"cluster_scores\"]))\n",
    "    if \"nonhope_score\" in p:\n",
    "        p[\"nonhope_score\"] = float(p[\"nonhope_score\"])\n",
    "    return p\n",
    "\n",
    "json_ready = [convert_place_for_json(p) for p in all_places]\n",
    "\n",
    "with open(\"all_places_embedding.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_ready, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"all_places_embedding.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_places = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d4b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import time, datetime, timedelta\n",
    "\n",
    "# ìœ í´ë¦¬ë“œ ê¸°ë°˜ ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜\n",
    "def euclidean(lat1, lon1, lat2, lon2):\n",
    "    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n",
    "\n",
    "# ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜\n",
    "def compute_distance_matrix(places): \n",
    "    n = len(places)\n",
    "    matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                matrix[i][j] = euclidean(\n",
    "                    places[i]['lat'], places[i]['lng'],\n",
    "                    places[j]['lat'], places[j]['lng']\n",
    "                )\n",
    "    return matrix\n",
    "\n",
    "# ë¶„ë‹¨ìœ„ ì‹œê°„ ì°¨ì´ ê³„ì‚° í•¨ìˆ˜\n",
    "def time_diff_minutes(t1, t2):\n",
    "    dt1 = datetime.combine(datetime.today(), t1)\n",
    "    dt2 = datetime.combine(datetime.today(), t2)\n",
    "    return abs((dt1 - dt2).total_seconds() / 60)\n",
    "\n",
    "time_table = []\n",
    "\n",
    "class ScheduleItem:\n",
    "    def __init__(self, title, start, end, place_type):\n",
    "        self.title = title\n",
    "        self.start = start  \n",
    "        self.end = end     \n",
    "        self.place_type = place_type\n",
    "\n",
    "# ëª…ì†Œ ì œì•½ ì¡°ê±´ ëŒ€ì…\n",
    "def get_constraints(base_mode=\"ëª…ì†Œ ì¤‘ì‹¬\"):\n",
    "    constraints = {\n",
    "        \"must_visit_attraction_every_minutes\": 180,  # 3ì‹œê°„ : ë§ˆì§€ë§‰ ëª…ì†Œ ë°©ë¬¸ í›„ ì¼ì • ì‹œê°„ì´ ì§€ë‚˜ë©´ ë°˜ë“œì‹œ ìƒˆë¡œìš´ ëª…ì†Œë¥¼ ì¶”ì²œí•´ì•¼ í•œë‹¤ëŠ” ì œì•½\n",
    "        \"attraction_required\": True, # í•˜ë£¨ ì¼ì •ì— ëª…ì†Œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ì—¬ë¶€\n",
    "\n",
    "        \"min_minutes_between_meals\": 360,  # 6ì‹œê°„ : ì‹ì‚¬ í›„ ì‹ì‚¬ë¥¼ ë°˜ë“œì‹œ í•´ì•¼í•˜ëŠ” ì œì•½ ì—¬ë¶€\n",
    "        \"require_meal_after_threshold\": True, # ì‹ì‚¬ë¥¼ í•˜ì§€ ì•Šê³  ì¼ì • ì‹œê°„ì´ ì§€ë‚˜ë©´, ë‹¤ìŒ ì¼ì •ìœ¼ë¡œ ë°˜ë“œì‹œ ì‹ì‚¬ë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤ëŠ” ì¡°ê±´\n",
    "        \"dont_eat_meal\": 180, #ì‹ì‚¬ í›„ ì§€ë‚˜ê°€ì•¼í•˜ëŠ” ì‹œê°„ ì—¬ë¶€\n",
    "\n",
    "        \"department_store_required_interval\": None, # ì¼ì •ì´ íŠ¹ì • ì‹œê°„(ë¶„ ë‹¨ìœ„) ì´ìƒ ì§€ë‚  ë•Œë§ˆë‹¤ ë°±í™”ì ì´ë‚˜ ì‡¼í•‘ëª°ì„ ê¼­ ì¼ì •ì— ë„£ì–´ì•¼ í•œë‹¤ëŠ” ì œì•½ ì¡°ê±´\n",
    "\n",
    "        \"allow_multiple_cafes\": False # í•˜ë£¨ ì¼ì •ì— ì¹´í˜(ë˜ëŠ” ìœ ì‚¬ ì¥ì†Œ: ë¹µì§‘ ë“±)ë¥¼ ì—°ì†ìœ¼ë¡œ í¬í•¨í•˜ëŠ” ê²ƒì„ í—ˆìš©í• ì§€ ì—¬ë¶€\n",
    "    }\n",
    "    # ì¶”ê°€ ì„ íƒ ì˜µì…˜ ë°˜ì˜\n",
    "    if base_mode ==\"ì‹ì‚¬ ì¤‘ì‹¬\": # ëª…ì†Œ í•„ìˆ˜ ì œê±°í•˜ê³  ì‹ì‚¬ ê°€ê²Œ ì œì•½ì„ 3ì‹œê°„ìœ¼ë¡œ ê°ì†Œ\n",
    "        constraints[\"attraction_required\"] = False\n",
    "        constraints[\"min_minutes_between_meals\"] = 180\n",
    "\n",
    "    if base_mode ==\"ì¹´í˜, ë¹µì§‘ ì¤‘ì‹¬\": # ì‹ì‚¬ í•„ìˆ˜ ì¡°ê±´ ì œê±°í•˜ê³  ëª…ì†Œ í•„ìˆ˜ ì œê±°, ì¹´í˜ì—°ì† í—ˆìš©\n",
    "        constraints[\"require_meal_after_threshold\"] = False\n",
    "        constraints[\"attraction_required\"] = False\n",
    "        constraints[\"allow_multiple_cafes\"] = True\n",
    "\n",
    "    if base_mode ==\"ì‡¼í•‘ ì¤‘ì‹¬\": # ëª…ì†Œì¡°ê±´ ì œê±° ë° ë°±í™”ì  ê°œìˆ˜ ì œí•œ í•´ì œ\n",
    "        constraints[\"department_store_required_interval\"] = 180  # 3ì‹œê°„ë§ˆë‹¤ ì‡¼í•‘\n",
    "        constraints[\"attraction_required\"] = False\n",
    "\n",
    "    return constraints\n",
    "\n",
    "# ì¸ë±ìŠ¤ ê¸°ì¤€ ì§€ë‚œ ìµœëŒ€ ì‹œê°„ ë°˜í™˜\n",
    "def get_elapsed_minutes_since_last_type(place_type, time_table, idx):\n",
    "    \"\"\"í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€, í•´ë‹¹ íƒ€ì…(place_type)ì˜ ë§ˆì§€ë§‰ ë°©ë¬¸ìœ¼ë¡œë¶€í„° ê²½ê³¼í•œ ì‹œê°„(ë¶„)ì„ ë°˜í™˜\"\"\"\n",
    "    current_start = time_table[idx].start\n",
    "\n",
    "    for i in range(idx - 1, -1, -1):  # í˜„ì¬ index ì´ì „ë§Œ íƒìƒ‰\n",
    "        if time_table[i].place_type == place_type:\n",
    "            last_end = time_table[i].end\n",
    "            return time_diff_minutes(last_end, current_start)\n",
    "\n",
    "    return None  # í•´ë‹¹ íƒ€ì…ì´ ì•„ì˜ˆ ì—†ì—ˆë˜ ê²½ìš°\n",
    "\n",
    "# íƒ€ì… ì„ íƒ í•¨ìˆ˜\n",
    "def select_allowed_types(time_table, base_mode, idx):\n",
    "    allowed_types = ['tourist_attraction', 'cafe', 'restaurant', 'bakery', 'bar', 'shopping_mall']\n",
    "\n",
    "    constraints = get_constraints(base_mode)\n",
    "\n",
    "    if constraints[\"attraction_required\"] == True:\n",
    "        if get_elapsed_minutes_since_last_type('tourist_attraction', time_table, idx)>=constraints[\"must_visit_attraction_every_minutes\"]:\n",
    "            allowed_types = ['tourist_attraction']\n",
    "            return allowed_types\n",
    "    \n",
    "    if constraints[\"require_meal_after_threshold\"] == True:\n",
    "        if get_elapsed_minutes_since_last_type('restaurant', time_table, idx)<=constraints[\"dont_eat_meal\"]:\n",
    "            allowed_types.remove(\"restaurant\")\n",
    "        if get_elapsed_minutes_since_last_type('restaurant', time_table, idx)>=constraints[\"min_minutes_between_meals\"]:\n",
    "            allowed_types = ['restaurant']\n",
    "            return allowed_types\n",
    "    \n",
    "    if constraints[\"department_store_required_interval\"] != None:\n",
    "        if get_elapsed_minutes_since_last_type('shopping_mall', time_table, idx)<=constraints[\"department_store_required_interval\"]:\n",
    "            allowed_types = ['shopping_mall']\n",
    "            return allowed_types\n",
    "    \n",
    "    if constraints[\"allow_multiple_cafes\"] == False:\n",
    "        if time_table[idx-1].place_type == \"cafe\" or time_table[idx-1].place_type == \"bakery\":\n",
    "            for t in [\"cafe\", \"bakery\"]:\n",
    "                allowed_types.remove(t)\n",
    "    \n",
    "    return allowed_types\n",
    "\n",
    "def generate_empty_slots(time_table, day_start=time(9, 0), day_end=time(23, 59)):\n",
    "    empty_slots = []\n",
    "\n",
    "    def to_datetime(t):\n",
    "        return datetime.combine(datetime.today(), t)\n",
    "\n",
    "    # ì •ë ¬ëœ íƒ€ì„í…Œì´ë¸”ë¡œ ê°€ì •\n",
    "    sorted_table = sorted(time_table, key=lambda x: x.start)\n",
    "\n",
    "    # Step 1. ì²˜ìŒ ~ ì²« ì¼ì • ì „ êµ¬ê°„\n",
    "    if not sorted_table or sorted_table[0].start > day_start:\n",
    "        empty_slots += split_empty_range(day_start, sorted_table[0].start if sorted_table else day_end)\n",
    "\n",
    "    # Step 2. ì¼ì • ì‚¬ì´ ë¹ˆ ê³µê°„ ì°¾ê¸°\n",
    "    for i in range(len(sorted_table) - 1):\n",
    "        current_end = sorted_table[i].end\n",
    "        next_start = sorted_table[i + 1].start\n",
    "        if current_end < next_start:\n",
    "            empty_slots += split_empty_range(current_end, next_start)\n",
    "\n",
    "    # Step 3. ë§ˆì§€ë§‰ ì¼ì • ~ í•˜ë£¨ ë\n",
    "    if sorted_table and sorted_table[-1].end < day_end:\n",
    "        empty_slots += split_empty_range(sorted_table[-1].end, day_end)\n",
    "\n",
    "    return empty_slots\n",
    "\n",
    "def split_empty_range(start_time, end_time):\n",
    "    slots = []\n",
    "    dt_start = datetime.combine(datetime.today(), start_time)\n",
    "    dt_end = datetime.combine(datetime.today(), end_time)\n",
    "    gap_minutes = int((dt_end - dt_start).total_seconds() // 60)\n",
    "\n",
    "    if gap_minutes < 90:\n",
    "        return []  # ë„ˆë¬´ ì§§ì€ ê³µê°„ì€ ë¬´ì‹œ\n",
    "\n",
    "    elif gap_minutes < 180:\n",
    "        # í•˜ë‚˜ë¡œ í†µì§œ ìŠ¬ë¡¯\n",
    "        slots.append(ScheduleItem(None, start_time, end_time, None))\n",
    "    else:\n",
    "        # 90ë¶„ ë‹¨ìœ„ë¡œ ìª¼ê°¬\n",
    "        dt_cursor = dt_start\n",
    "        while (dt_end - dt_cursor).total_seconds() >= 90 * 60:\n",
    "            dt_next = dt_cursor + timedelta(minutes=90)\n",
    "            slots.append(ScheduleItem(None, dt_cursor.time(), dt_next.time(), None))\n",
    "            dt_cursor = dt_next\n",
    "\n",
    "        # ë‚¨ì€ ì‹œê°„ (ì˜ˆ: 2ì‹œê°„ 30ë¶„ì—ì„œ ë§ˆì§€ë§‰ 30ë¶„)\n",
    "        if dt_cursor < dt_end:\n",
    "            slots.append(ScheduleItem(None, dt_cursor.time(), dt_end.time(), None))\n",
    "\n",
    "    return slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cacbe8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:00 - 11:30 | ê²½ë³µê¶ | tourist_attraction\n",
      "11:30 - 13:00 | (ë¹ˆ ìŠ¬ë¡¯) | -\n",
      "13:00 - 14:00 | ì ì‹¬ | restaurant\n",
      "14:00 - 16:00 | (ë¹ˆ ìŠ¬ë¡¯) | -\n",
      "16:00 - 18:00 | Níƒ€ì›Œ | tourist_attraction\n",
      "18:00 - 19:30 | (ë¹ˆ ìŠ¬ë¡¯) | -\n",
      "19:30 - 21:00 | (ë¹ˆ ìŠ¬ë¡¯) | -\n",
      "21:00 - 22:30 | (ë¹ˆ ìŠ¬ë¡¯) | -\n",
      "22:30 - 23:59 | (ë¹ˆ ìŠ¬ë¡¯) | -\n"
     ]
    }
   ],
   "source": [
    "time_table = [\n",
    "    ScheduleItem(\"ê²½ë³µê¶\", time(10, 0), time(11, 30), \"tourist_attraction\"),\n",
    "    ScheduleItem(\"ì ì‹¬\", time(13, 0), time(14, 0), \"restaurant\"),\n",
    "    ScheduleItem(\"Níƒ€ì›Œ\", time(16, 0), time(18, 0), \"tourist_attraction\"),\n",
    "]\n",
    "\n",
    "# 1. ë¹ˆ ìŠ¬ë¡¯ ìƒì„±\n",
    "empty_slots = generate_empty_slots(time_table)\n",
    "\n",
    "# 2. ì‹¤ì œ ì¼ì •ê³¼ ë¹ˆ ìŠ¬ë¡¯ì„ í•©ì¹¨\n",
    "full_schedule = time_table + empty_slots\n",
    "\n",
    "# 3. ì‹œì‘ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "full_schedule.sort(key=lambda x: x.start)\n",
    "\n",
    "# 4. ì¶œë ¥\n",
    "for item in full_schedule:\n",
    "    title = item.title if item.title else \"(ë¹ˆ ìŠ¬ë¡¯)\"\n",
    "    type_ = item.place_type if item.place_type else \"-\"\n",
    "    print(f\"{item.start.strftime('%H:%M')} - {item.end.strftime('%H:%M')} | {title} | {type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a10c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
