{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyBEl50H0xV7SnyNwcc0Yo-Ru-iiTXTBePc\"\n",
    "\n",
    "PLACE_TYPES = [\n",
    "    \"tourist_attraction\",\n",
    "    \"cafe\",\n",
    "    \"bar\",\n",
    "    \"bakery\",\n",
    "    \"restaurant\",\n",
    "    \"shopping_mall\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22784cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” 'ì‹ ë„ë¦¼ì—­' ì£¼ë³€ ê²€ìƒ‰ ê²°ê³¼:\n",
      "\n",
      "ğŸ”¹ Tourist_Attraction (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ):\n",
      "\n",
      "ğŸ”¹ Cafe (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ):\n",
      "\n",
      "ğŸ”¹ Bar (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ):\n",
      "\n",
      "ğŸ”¹ Bakery (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ):\n",
      "\n",
      "ğŸ”¹ Restaurant (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ):\n",
      "\n",
      "ğŸ”¹ Shopping_Mall (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ):\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import math\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "query = input(\"ğŸ“ ì–´ë–¤ ìœ„ì¹˜ ì£¼ë³€ì„ ì°¾ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? (ì˜ˆ: ê²½ë³µê¶, ê°•ë‚¨ì—­): \\n\")\n",
    "method = int(input(\"ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì—¬í–‰í•˜ì‹œë‚˜ìš”? 1 : ë„ë³´ 2 : ëŒ€ì¤‘êµí†µ, 3 : ì§ì ‘ ìš´ì „\\n\"))\n",
    "radius = {1: \"2000\", 2: \"10000\", 3: \"20000\"}.get(method)\n",
    "\n",
    "# ì§€ì˜¤ì½”ë”©\n",
    "geo_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "geo_params = {\n",
    "    \"address\": query,\n",
    "    \"key\": API_KEY,\n",
    "    \"language\": \"ko\"\n",
    "}\n",
    "geo_res = requests.get(geo_url, params=geo_params).json()\n",
    "\n",
    "if not geo_res[\"results\"]:\n",
    "    print(\"ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "location = geo_res[\"results\"][0][\"geometry\"][\"location\"]\n",
    "lat, lng = location[\"lat\"], location[\"lng\"]\n",
    "\n",
    "# ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜\n",
    "def compute_trust_score(rating, reviews, latest_review_time_str=\"\"):\n",
    "    if rating is None or reviews is None:\n",
    "        return 0\n",
    "    base_score = rating * math.log(reviews + 1)\n",
    "    bonus_ratio = 0.0\n",
    "    try:\n",
    "        if \"day\" in latest_review_time_str or \"week\" in latest_review_time_str:\n",
    "            bonus_ratio = 0.10\n",
    "        elif \"month\" in latest_review_time_str:\n",
    "            months = int(latest_review_time_str.split()[0])\n",
    "            if months <= 1:\n",
    "                bonus_ratio = 0.10\n",
    "            elif months <= 6:\n",
    "                bonus_ratio = 0.05\n",
    "    except:\n",
    "        bonus_ratio = 0.0\n",
    "    return round(base_score * (1 + bonus_ratio), 2)\n",
    "\n",
    "# ì¥ì†Œ ê²€ìƒ‰ (ë¦¬ë·°ëŠ” ì•ˆ ë°›ìŒ)\n",
    "def search_places_basic(place_type):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "    params = {\n",
    "        \"location\": f\"{lat},{lng}\",\n",
    "        \"radius\": radius,\n",
    "        \"type\": place_type,\n",
    "        \"language\": \"ko\",\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "\n",
    "    candidates = []\n",
    "    for _ in range(2):  # ìµœëŒ€ 2í˜ì´ì§€\n",
    "        res = requests.get(url, params=params).json()\n",
    "        results = res.get(\"results\", [])\n",
    "        for place in results:\n",
    "            rating = place.get(\"rating\", 0)\n",
    "            user_ratings_total = place.get(\"user_ratings_total\", 0)\n",
    "            if user_ratings_total < 1 or rating < 3.5:\n",
    "                continue\n",
    "            location = place.get(\"geometry\", {}).get(\"location\", {})\n",
    "            place_lat = location.get(\"lat\")\n",
    "            place_lng = location.get(\"lng\")\n",
    "            candidates.append({\n",
    "                \"place_id\": place.get(\"place_id\"),\n",
    "                \"name\": place.get(\"name\"),\n",
    "                \"vicinity\": place.get(\"vicinity\", \"ì£¼ì†Œ ì—†ìŒ\"),\n",
    "                \"rating\": rating,\n",
    "                \"user_ratings_total\": user_ratings_total,\n",
    "                \"trust_score\": compute_trust_score(rating, user_ratings_total),\n",
    "                \"type\": place_type,\n",
    "                \"lat\": place_lat,\n",
    "                \"lng\": place_lng\n",
    "            })\n",
    "        token = res.get(\"next_page_token\")\n",
    "        if not token:\n",
    "            break\n",
    "        time.sleep(2)\n",
    "        params = {\"pagetoken\": token, \"key\": API_KEY, \"language\": \"ko\"}\n",
    "    # ìƒìœ„ 20ê°œë§Œ ë°˜í™˜\n",
    "    candidates.sort(key=lambda x: x[\"trust_score\"], reverse=True)\n",
    "    return candidates[:40]\n",
    "\n",
    "# ë¦¬ë·° ìš”ì²­ í•¨ìˆ˜\n",
    "def get_reviews_and_business_info(place_id):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"review,business_status,opening_hours\",\n",
    "        \"language\": \"ko\",\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "    res = requests.get(url, params=params).json()\n",
    "    result = res.get(\"result\", {})\n",
    "\n",
    "    # ë¦¬ë·°\n",
    "    reviews = result.get(\"reviews\", [])\n",
    "    texts = [r[\"text\"] for r in reviews[:5]]\n",
    "    latest_time = reviews[0][\"relative_time_description\"] if reviews else \"\"\n",
    "\n",
    "    # ì˜ì—… ìƒíƒœ\n",
    "    business_status = result.get(\"business_status\", \"UNKNOWN\")\n",
    "\n",
    "    # ìš´ì˜ ì‹œê°„\n",
    "    opening_hours = result.get(\"opening_hours\", {})\n",
    "    open_now = opening_hours.get(\"open_now\", None)\n",
    "    weekday_text = opening_hours.get(\"weekday_text\", [])\n",
    "\n",
    "    return texts, latest_time, business_status, open_now, weekday_text\n",
    "\n",
    "# ì „ì²´ ì¥ì†Œ ëˆ„ì  ì €ì¥\n",
    "all_places = []\n",
    "\n",
    "print(f\"\\nğŸ” '{query}' ì£¼ë³€ ê²€ìƒ‰ ê²°ê³¼:\")\n",
    "\n",
    "for place_type in PLACE_TYPES:\n",
    "    top_places = search_places_basic(place_type)\n",
    "    print(f\"\\nğŸ”¹ {place_type.title()} (ì‹ ë¢°ë„ ìƒìœ„ 20ê°œ)\")\n",
    "    for place in top_places:\n",
    "        reviews, latest_time, biz_status, open_now, weekday_hours = get_reviews_and_business_info(place[\"place_id\"])\n",
    "        place[\"reviews\"] = reviews\n",
    "        place[\"trust_score\"] = compute_trust_score(place[\"rating\"], place[\"user_ratings_total\"], latest_time)\n",
    "        place[\"business_status\"] = biz_status\n",
    "        place[\"open_now\"] = open_now\n",
    "        place[\"weekday_text\"] = weekday_hours\n",
    "        all_places.append(place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b433bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"all_places.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_places, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216d6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"all_places.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_places = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43a2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_review(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = text.strip()\n",
    "\n",
    "    if len(text) < 3:\n",
    "        return None\n",
    "\n",
    "    if not re.search(\"[ê°€-í£]\", text):  # í•œê¸€ ì—†ëŠ” ì™¸êµ­ì–´ ë¦¬ë·° ì œê±°\n",
    "        return None\n",
    "\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # ë°˜ë³µ ë¬¸ì ì •ë¦¬\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£.,!?]\", \"\", text)  # ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = text[:300]  # ë„ˆë¬´ ê¸´ ë¦¬ë·° ìë¥´ê¸°\n",
    "\n",
    "    return text if text else None\n",
    "\n",
    "for place in all_places:\n",
    "    original_reviews = place.get(\"reviews\", [])\n",
    "    cleaned_reviews = []\n",
    "\n",
    "    for review in original_reviews:\n",
    "        cleaned = clean_review(review)\n",
    "        if cleaned:\n",
    "            cleaned_reviews.append(cleaned)\n",
    "\n",
    "    place[\"reviews\"] = cleaned_reviews  # âœ… ì „ì²˜ë¦¬ëœ ë¦¬ë·°ë¡œ ë®ì–´ì“°ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—¬í–‰ì—ì„œì˜ í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œ : ì¢…ë£Œ\n",
      "ì—¬í–‰ì—ì„œì˜ ë¹„í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œ : ì¢…ë£Œ\n"
     ]
    }
   ],
   "source": [
    "print(\"ì—¬í–‰ì—ì„œì˜ í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì˜ˆ : ì „ë§ëŒ€ ê³µì› ìì—°\")\n",
    "\"\"\"keyword_hope = []\n",
    "for i in range(10):\n",
    "    keyword = input()\n",
    "    if (keyword == \"ì¢…ë£Œ\"):\n",
    "        break\n",
    "    keyword_hope.append(keyword)\"\"\"\n",
    "\n",
    "print(\"ì—¬í–‰ì—ì„œì˜ ë¹„í¬ë§ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”! ì˜ˆ : ì‹œë„ëŸ¬ì›€ í˜¼ì¡\")\n",
    "\"\"\"keyword_nonhope = []\n",
    "for i in range(10):\n",
    "    keyword = input()\n",
    "    if (keyword == \"ì¢…ë£Œ\"):\n",
    "        break\n",
    "    keyword_nonhope.append(keyword)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac61e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (í•œ ì¤„ë¡œ ë)\n",
    "model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_sbert_embedding(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "    return model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "def get_sbert_review_vector(reviews):\n",
    "    embeddings = [\n",
    "        get_sbert_embedding(review)\n",
    "        for review in reviews if review.strip()\n",
    "    ]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "def get_place_vector_with_name(place, review_weight=1.0, name_weight=0):\n",
    "    reviews = place.get(\"reviews\", [])\n",
    "    name = place.get(\"name\", \"\")\n",
    "\n",
    "    review_vec = get_sbert_review_vector(reviews)\n",
    "    name_vec = get_sbert_embedding(name)\n",
    "\n",
    "    total_weight = review_weight + name_weight\n",
    "    final_vec = (review_weight * review_vec + name_weight * name_vec) / total_weight\n",
    "    return final_vec\n",
    "\n",
    "# ğŸ” ë¬¸ë§¥ ë²¡í„° ìƒì„± ë° ì €ì¥\n",
    "for place in all_places:\n",
    "    if \"reviews\" in place and \"name\" in place:\n",
    "        place[\"review_vector\"] = get_place_vector_with_name(place)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff39fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_hope = [\"ë£¨í”„íƒ‘\",\"ì•¼ê²½\",\"ì „ë§ëŒ€\",\"ê³ ì¸µê±´ë¬¼\",\"íŠ¸ëœë“œ ì¹´í˜\",\"íŒŒí‹°\",\"ë°”\",\"ì£¼ë¥˜\",\"ê³µì›\",\"í•œê°• ë·°\"]\n",
    "keyword_nonhope = [\"ë²Œë ˆ\",\"ë¨¼ì§€\",\"í˜¼ì¡\",\"ë”ëŸ¬ì›€\"]\n",
    "\n",
    "def clean_keyword(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = text.strip()\n",
    "\n",
    "    if not re.search(\"[ê°€-í£]\", text):  # í•œê¸€ ì—†ëŠ” ì™¸êµ­ì–´ ë¦¬ë·° ì œê±°\n",
    "        return None\n",
    "\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # ë°˜ë³µ ë¬¸ì ì •ë¦¬\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£.,!?]\", \"\", text)  # ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = text[:300]  # ë„ˆë¬´ ê¸´ ë¦¬ë·° ìë¥´ê¸°\n",
    "\n",
    "    return text if text else None\n",
    "\n",
    "keyword_hope = [clean_keyword(r) for r in keyword_hope if r]\n",
    "keyword_nonhope = [clean_keyword(r) for r in keyword_nonhope if r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018252c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyi1102\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['íŠ¸ëœë“œ ì¹´í˜', 'íŒŒí‹°', 'ë°”', 'ì£¼ë¥˜'], ['ì•¼ê²½', 'ì „ë§ëŒ€', 'ê³µì›', 'í•œê°• ë·°'], ['ë£¨í”„íƒ‘', 'ê³ ì¸µê±´ë¬¼']]\n",
      "['ë²Œë ˆ', 'ë¨¼ì§€', 'í˜¼ì¡', 'ë”ëŸ¬ì›€']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# 1. í¬ë§ í‚¤ì›Œë“œ ì„ë² ë”©\n",
    "hope_embeddings = [get_sbert_embedding(k) for k in keyword_hope if k]\n",
    "\n",
    "# 2. ìµœì  K íƒìƒ‰\n",
    "best_k = 2\n",
    "best_score = -1\n",
    "for k in range(2, min(len(hope_embeddings), 6)):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\").fit(hope_embeddings)\n",
    "    score = silhouette_score(hope_embeddings, kmeans.labels_)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "\n",
    "# 3. ìµœì  Kë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰\n",
    "final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=\"auto\").fit(hope_embeddings)\n",
    "labels = final_kmeans.labels_\n",
    "\n",
    "clustered_keywords_list = [[] for _ in range(best_k)]\n",
    "cluster_centroids = []\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° IDë³„ í‚¤ì›Œë“œì™€ ë²¡í„° ìˆ˜ì§‘\n",
    "cluster_vectors = [[] for _ in range(best_k)]\n",
    "\n",
    "for keyword, label in zip(keyword_hope, labels):\n",
    "    clustered_keywords_list[label].append(keyword)\n",
    "    cluster_vectors[label].append(get_sbert_embedding(keyword))  # ê° í‚¤ì›Œë“œì˜ ë²¡í„° ì €ì¥\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ë²¡í„° ê³„ì‚°\n",
    "for vectors in cluster_vectors:\n",
    "    if vectors:\n",
    "        centroid = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        centroid = np.zeros(model.get_sentence_embedding_dimension())\n",
    "    cluster_centroids.append(centroid)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "keyword_hope = clustered_keywords_list  # [[cluster1 í‚¤ì›Œë“œë“¤], [cluster2 í‚¤ì›Œë“œë“¤], ...]\n",
    "keyword_hope_centroids = cluster_centroids  # [cluster1 ë²¡í„°, cluster2 ë²¡í„°, ...]\n",
    "\n",
    "print(keyword_hope)\n",
    "print(keyword_nonhope)\n",
    "\n",
    "# ë¹„ì„ í˜¸ í‚¤ì›Œë“œ ì„ë² ë”©\n",
    "nonhope_embeddings = [get_sbert_embedding(k) for k in keyword_nonhope if k]\n",
    "\n",
    "# í‰ê·  ì„ë² ë”© (ë¹ˆ ê²½ìš° ëŒ€ë¹„)\n",
    "if nonhope_embeddings:\n",
    "    nonhope_mean_vector = np.mean(nonhope_embeddings, axis=0)\n",
    "else:\n",
    "    nonhope_mean_vector = np.zeros(model.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31421b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cluster_scores(review_vector, name_vector, cluster_centroids, alpha=0.2):\n",
    "    scores = []\n",
    "    for centroid in cluster_centroids:\n",
    "        # ê° ìœ ì‚¬ë„ ê°œë³„ ê³„ì‚°\n",
    "        sim_review = (\n",
    "            cosine_similarity([review_vector], [centroid])[0][0]\n",
    "            if np.linalg.norm(review_vector) > 0 and np.linalg.norm(centroid) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        sim_name = (\n",
    "            cosine_similarity([name_vector], [centroid])[0][0]\n",
    "            if np.linalg.norm(name_vector) > 0 and np.linalg.norm(centroid) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "        # ì´ë¦„ê³¼ ë¦¬ë·°ì˜ ê°€ì¤‘í•©\n",
    "        score = (1 - alpha) * sim_review + alpha * sim_name\n",
    "        scores.append(round(score, 4))\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ëª¨ë“  ì¥ì†Œì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„° ì ìˆ˜ ë¶€ì—¬\n",
    "for place in all_places:\n",
    "    review_vec = place.get(\"review_vector\")\n",
    "    name_vec = get_sbert_embedding(place.get(\"name\", \"\"))\n",
    "    if review_vec is not None:\n",
    "        place[\"cluster_scores\"] = compute_cluster_scores(\n",
    "            review_vec, name_vec, keyword_hope_centroids, alpha=0.2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edae500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Type: tourist_attraction\n",
      "  - í‘ì„ë™ê³µì› | ì ìˆ˜: [0.24, 0.61, 0.23]\n",
      "  - ì‚°ê¸°ìŠ­ê³µì› | ì ìˆ˜: [0.13, 0.55, 0.22]\n",
      "  - ì„œë˜ì„¬ | ì ìˆ˜: [0.19, 0.54, 0.18]\n",
      "  - ë§ˆì„ìˆ²ê³µì› | ì ìˆ˜: [0.21, 0.53, 0.18]\n",
      "  - ì›íš¨ëŒ€êµ | ì ìˆ˜: [0.22, 0.52, 0.35]\n",
      "  - ê°€ë¡œê³µì› | ì ìˆ˜: [0.28, 0.52, 0.29]\n",
      "  - ë•ìˆ˜ê³µì› | ì ìˆ˜: [0.20, 0.48, 0.23]\n",
      "  - ê±°ë¦¬ê³µì› | ì ìˆ˜: [0.12, 0.48, 0.12]\n",
      "  - ê³„ë‚¨ì œ1ê·¼ë¦°ê³µì› | ì ìˆ˜: [0.13, 0.48, 0.15]\n",
      "  - ê´€ì•…ì‚° ìì—°ê³µì› | ì ìˆ˜: [0.12, 0.46, 0.09]\n",
      "\n",
      "ğŸ“‚ Type: cafe\n",
      "  - ë™ì‘ë…¸ì„ì¹´í˜ | ì ìˆ˜: [0.24, 0.46, 0.25]\n",
      "  - ìŠ¤ë¬´ë””í‚¹ ì˜ë“±í¬íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.44, 0.24, 0.16]\n",
      "  - íˆ¬ì¸í”Œë ˆì´ìŠ¤ ì„œê°•ëŒ€ì  | ì ìˆ˜: [0.41, 0.33, 0.19]\n",
      "  - í• ë¦¬ìŠ¤ ì»¤í”¼ êµ¬ë¡œì  | ì ìˆ˜: [0.40, 0.25, 0.21]\n",
      "  - íˆ¬ì¸í”Œë ˆì´ìŠ¤ ê°€ì‚°ë””ì§€í„¸ì  | ì ìˆ˜: [0.37, 0.24, 0.14]\n",
      "  - ì‹ ì´Œë¯¸í”Œ | ì ìˆ˜: [0.36, 0.30, 0.14]\n",
      "  - ë¹„í•˜ì¸ë“œ | ì ìˆ˜: [0.35, 0.23, 0.11]\n",
      "  - ì–´ë°˜íŠ¸ë¦¬ | ì ìˆ˜: [0.34, 0.35, 0.22]\n",
      "  - ì•„í”„ë¦¬ì¹´ | ì ìˆ˜: [0.34, 0.24, 0.24]\n",
      "  - ì•„ë¦¬ìŠ¤íƒ€ì»¤í”¼ ë“±ì´Œì  | ì ìˆ˜: [0.34, 0.24, 0.17]\n",
      "\n",
      "ğŸ“‚ Type: bar\n",
      "  - ì¹´ìŠ¤íƒ€ìš´ | ì ìˆ˜: [0.46, 0.29, 0.18]\n",
      "  - ë¸”ë£¨ë²„ë“œ | ì ìˆ˜: [0.46, 0.31, 0.19]\n",
      "  - ì™€ë°” ì—¬ì˜ë„ ì§ì˜ì  | ì ìˆ˜: [0.43, 0.31, 0.18]\n",
      "  - ì¹˜ì–´ìŠ¤ì˜ë“±í¬êµ¬ì²­ì  | ì ìˆ˜: [0.42, 0.35, 0.21]\n",
      "  - ìƒ¤ë„¬ | ì ìˆ˜: [0.41, 0.35, 0.21]\n",
      "  - ì§íƒœ&ë…¸ê°€ë¦¬ | ì ìˆ˜: [0.39, 0.22, 0.16]\n",
      "  - í›Œë„ë¼ìˆ¯ë¶ˆë°”ë² í | ì ìˆ˜: [0.38, 0.23, 0.20]\n",
      "  - ë˜ë˜ì˜¤ë˜ | ì ìˆ˜: [0.38, 0.22, 0.13]\n",
      "  - ì¹ ì„±í¬ì°¨ | ì ìˆ˜: [0.38, 0.31, 0.15]\n",
      "  - ì¸ë””ìŠ¤ | ì ìˆ˜: [0.37, 0.33, 0.16]\n",
      "\n",
      "ğŸ“‚ Type: bakery\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ì—¬ì˜ë„ì  | ì ìˆ˜: [0.41, 0.32, 0.21]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ êµ¬ë¡œì—˜ë¦¼ì  | ì ìˆ˜: [0.39, 0.29, 0.25]\n",
      "  - ë˜í‚¨ë„ë„ˆì¸  ì—¼ì°½ì  | ì ìˆ˜: [0.37, 0.13, 0.21]\n",
      "  - ë˜í‚¨ í™ëŒ€ì—­ì  | ì ìˆ˜: [0.36, 0.25, 0.16]\n",
      "  - íŒŒë¦¬í¬ë¼ìƒ ì´ì´Œì  | ì ìˆ˜: [0.36, 0.24, 0.19]\n",
      "  - íŒŒë¦¬í¬ë¼ìƒ ì—¬ì˜ë„2í˜¸ì  | ì ìˆ˜: [0.34, 0.18, 0.16]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ì—¼ì°½ì  | ì ìˆ˜: [0.34, 0.17, 0.17]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ë‚œê³¡ì‚¬ê±°ë¦¬ì  | ì ìˆ˜: [0.33, 0.22, 0.16]\n",
      "  - íŒŒë¦¬ë°”ê²Œëœ¨ ì˜ë“±í¬ëŒ€ìš°ì  | ì ìˆ˜: [0.33, 0.24, 0.15]\n",
      "  - ëšœë ˆì¥¬ë¥´ ì—¬ì˜ì„œë¡œì  | ì ìˆ˜: [0.32, 0.19, 0.14]\n",
      "\n",
      "ğŸ“‚ Type: restaurant\n",
      "  - ìŠ¤ë¬´ë””í‚¹ ì˜ë“±í¬íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.44, 0.24, 0.16]\n",
      "  - ë¶€ì–´ì¹˜í‚¨ | ì ìˆ˜: [0.39, 0.29, 0.18]\n",
      "  - ë³¸ì£½ ì˜ë“±í¬ì‹œì¥ë¡œí„°ë¦¬ì  | ì ìˆ˜: [0.38, 0.23, 0.22]\n",
      "  - í”¼ìë‚˜ë¼ì¹˜í‚¨ê³µì£¼ êµ¬ë¡œ1í˜¸ì  | ì ìˆ˜: [0.37, 0.22, 0.17]\n",
      "  - ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤ êµ¬ë¡œê³ ì²™ | ì ìˆ˜: [0.35, 0.32, 0.23]\n",
      "  - ì¹˜í‚¨ë§¤ë‹ˆì•„ ì˜ë“±í¬ì—­ì  | ì ìˆ˜: [0.34, 0.27, 0.14]\n",
      "  - ì‚¿ë½€ë¡œ ëª©ë™ì  | ì ìˆ˜: [0.33, 0.25, 0.12]\n",
      "  - í•˜ë‚˜ìŠ¤ì‹œ íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.31, 0.13, 0.13]\n",
      "  - ì´ë ˆ | ì ìˆ˜: [0.29, 0.20, 0.11]\n",
      "  - í•œì¼ê´€ íƒ€ì„ìŠ¤í€˜ì–´ì  | ì ìˆ˜: [0.28, 0.20, 0.16]\n",
      "\n",
      "ğŸ“‚ Type: shopping_mall\n",
      "  - ë§¥ìŠ¤ì—”ì˜ | ì ìˆ˜: [0.49, 0.34, 0.28]\n",
      "  - ë”ìŠˆíŠ¸í•˜ìš°ìŠ¤ ì˜ë“±í¬í™ˆí”ŒëŸ¬ìŠ¤ | ì ìˆ˜: [0.34, 0.28, 0.16]\n",
      "  - TOUCH | ì ìˆ˜: [0.34, 0.31, 0.16]\n",
      "  - LUCE | ì ìˆ˜: [0.33, 0.24, 0.21]\n",
      "  - ì§€ì˜¤ì§€ì•„ êµ¬ë¡œì  | ì ìˆ˜: [0.33, 0.22, 0.23]\n",
      "  - YA | ì ìˆ˜: [0.29, 0.15, 0.09]\n",
      "  - ì„¸ì • íƒ€ì„ìŠ¤í€˜ì–´ì§€ì  | ì ìˆ˜: [0.19, 0.26, 0.17]\n",
      "  - ê¸ˆí™”ì£¼ë‹¨ | ì ìˆ˜: [0.07, 0.05, 0.07]\n",
      "  - ë² ë² ì•™ìŠˆ | ì ìˆ˜: [0.06, 0.02, 0.06]\n",
      "  - ì¿ ì•„ ì‹ ì„¸ê³„ì˜ë“±í¬ì§€ìƒA | ì ìˆ˜: [0.06, 0.06, 0.06]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# íƒ€ì…ë³„ ê·¸ë£¹í•‘\n",
    "type_grouped = defaultdict(list)\n",
    "for place in all_places:\n",
    "    if \"cluster_scores\" in place:\n",
    "        type_grouped[place[\"type\"]].append(place)\n",
    "\n",
    "# ì •ë ¬ ë° ì¶œë ¥\n",
    "for place_type, places in type_grouped.items():\n",
    "    print(f\"\\nğŸ“‚ Type: {place_type}\")\n",
    "    sorted_places = sorted(places, key=lambda x: max(x[\"cluster_scores\"]), reverse=True)\n",
    "    for p in sorted_places[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥\n",
    "        scores_str = \", \".join(f\"{s:.2f}\" for s in p[\"cluster_scores\"])\n",
    "        print(f\"  - {p['name']} | ì ìˆ˜: [{scores_str}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecce99a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¹„ì„ í˜¸ í‚¤ì›Œë“œ ìœ ì‚¬ë„ (íƒ€ì…ë³„ ìƒìœ„ 3ê°œì”©):\n",
      "\n",
      "ğŸ”¹ Tourist_Attraction:\n",
      "1. ê°€ë¡œê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3953 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 49\n",
      "2. ë§¤í™”ê·¼ë¦°ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3765 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 29\n",
      "3. ê¸ˆì²œêµ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3528 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 171\n",
      "4. ë°©í™”ìŒˆì§€ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3411 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 167\n",
      "5. ì„œë˜ì„¬\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3358 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 761\n",
      "6. í‘ì„ë™ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3261 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 97\n",
      "7. ë•ìˆ˜ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3092 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 51\n",
      "8. ì´í™”ì—¬ìëŒ€í•™êµ ìì—°ì‚¬ë°•ë¬¼ê´€\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3009 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 111\n",
      "9. ì›íš¨ëŒ€êµ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2819 | í‰ì : 4.2 | ë¦¬ë·° ìˆ˜: 104\n",
      "10. ì¦ì‚°ì²´ìœ¡ê³µì›\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2777 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 208\n",
      "\n",
      "ğŸ”¹ Cafe:\n",
      "1. íƒì•¤íƒìŠ¤ ì˜ë“±í¬ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4440 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 130\n",
      "2. ì‹ ì´Œë¯¸í”Œ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4241 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 91\n",
      "3. ì–´ë°˜íŠ¸ë¦¬\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4031 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 1\n",
      "4. ì»¤í”¼ë¹ˆ í™ëŒ€ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3678 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 557\n",
      "5. ì•„ë¦¬ìŠ¤íƒ€ì»¤í”¼ ë“±ì´Œì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3416 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 14\n",
      "6. í•¸ë“œí”½íŠ¸í˜¸í…”\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3290 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 599\n",
      "7. ë¹„í•˜ì¸ë“œ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3274 | í‰ì : 4.3 | ë¦¬ë·° ìˆ˜: 63\n",
      "8. í• ë¦¬ìŠ¤ ì»¤í”¼ êµ¬ë¡œì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3207 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 88\n",
      "9. í¬ì— \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3206 | í‰ì : 5 | ë¦¬ë·° ìˆ˜: 2\n",
      "10. íŒŒìŠ¤ì¿ ì°Œë³´ë¼ë§¤ê³µì›ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3131 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 203\n",
      "\n",
      "ğŸ”¹ Bar:\n",
      "1. ë¹„ë‹í•˜ìš°ìŠ¤\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.5079 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 148\n",
      "2. ë¸”ë£¨ë²„ë“œ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4519 | í‰ì : 4.6 | ë¦¬ë·° ìˆ˜: 5\n",
      "3. ì¸ë””ìŠ¤\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.4203 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 44\n",
      "4. ì´í™”ì£¼ë§‰\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3712 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 2\n",
      "5. MAG Live Club\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3710 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 3\n",
      "6. ì§íƒœ&ë…¸ê°€ë¦¬\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3609 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 15\n",
      "7. ì™€ë¼ì™€ë¼\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3600 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 114\n",
      "8. ì™€ë°” ì˜ë“±í¬ ì§ì˜ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3549 | í‰ì : 4.5 | ë¦¬ë·° ìˆ˜: 2\n",
      "9. ë¦¬ë¹ ë˜¥\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3456 | í‰ì : 4.2 | ë¦¬ë·° ìˆ˜: 22\n",
      "10. ë˜ë˜ì˜¤ë˜\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3232 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 4\n",
      "\n",
      "ğŸ”¹ Bakery:\n",
      "1. ë˜í‚¨ í™ëŒ€ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3713 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 159\n",
      "2. íŒŒë¦¬í¬ë¼ìƒ ì—¬ì˜ë„2í˜¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3304 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 310\n",
      "3. íŒŒë¦¬ë°”ê²Œëœ¨ ì˜¤ëª©êµì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3220 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 13\n",
      "4. íŒŒë¦¬ë°”ê²Œëœ¨ ì—¬ì˜ë„ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3211 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 25\n",
      "5. ë¹µêµ¼í„° ëŒ€ë¦¼ë™ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3209 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 1\n",
      "6. íŒŒë¦¬ë°”ê²Œëœ¨ êµ¬ë¡œë””ì§€í„¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3085 | í‰ì : 3.9 | ë¦¬ë·° ìˆ˜: 14\n",
      "7. ë˜í‚¨ë„ë„ˆì¸  ì—¼ì°½ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3079 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 23\n",
      "8. íŒŒë¦¬ë°”ê²Œëœ¨ ê¸ˆì²œêµ¬ì²­ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3008 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 15\n",
      "9. ë˜í‚¨ë„ë„ˆì¸  êµ¬ë¡œíƒœí‰ì–‘ë¬¼ì‚°ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2919 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 59\n",
      "10. íŒŒë¦¬ë°”ê²Œëœ¨ êµ¬ë¡œì—˜ë¦¼ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2871 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 27\n",
      "\n",
      "ğŸ”¹ Restaurant:\n",
      "1. ë³¸ì£½ ì˜ë“±í¬ì‹œì¥ë¡œí„°ë¦¬ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3665 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 11\n",
      "2. ì´ë ˆ\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3315 | í‰ì : 4.8 | ë¦¬ë·° ìˆ˜: 12\n",
      "3. ë°°ìŠ¤í‚¨ë¼ë¹ˆìŠ¤ êµ¬ë¡œê³ ì²™\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3303 | í‰ì : 4.4 | ë¦¬ë·° ìˆ˜: 51\n",
      "4. í•¸ë“œí”½íŠ¸í˜¸í…”\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3290 | í‰ì : 4.1 | ë¦¬ë·° ìˆ˜: 599\n",
      "5. í•˜ë‚˜ìŠ¤ì‹œ íƒ€ì„ìŠ¤í€˜ì–´ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3211 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 4\n",
      "6. ë¡¯ë°ë¦¬ì•„ êµ¬ë¡œì‹œì¥ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3129 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 321\n",
      "7. ì˜¤ëª©ì§‘ ëª©ë™ë³¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2981 | í‰ì : 4.2 | ë¦¬ë·° ìˆ˜: 421\n",
      "8. í•œì¼ê´€ íƒ€ì„ìŠ¤í€˜ì–´ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2942 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 410\n",
      "9. í”¼ìë‚˜ë¼ì¹˜í‚¨ê³µì£¼ êµ¬ë¡œ1í˜¸ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2681 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 15\n",
      "10. í”¼ììŠ¤ì¿¨ ì˜ë“±í¬ë‚¨ë¶€ì—­ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2624 | í‰ì : 4.5 | ë¦¬ë·° ìˆ˜: 72\n",
      "\n",
      "ğŸ”¹ Shopping_Mall:\n",
      "1. ì§€ì˜¤ì§€ì•„ êµ¬ë¡œì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3878 | í‰ì : 3.7 | ë¦¬ë·° ìˆ˜: 7\n",
      "2. LUCE\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.3819 | í‰ì : 3.5 | ë¦¬ë·° ìˆ˜: 6\n",
      "3. TOUCH\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2929 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 5\n",
      "4. í† íŠ¸ë¼\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2928 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 1\n",
      "5. YA\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2805 | í‰ì : 5 | ë¦¬ë·° ìˆ˜: 1\n",
      "6. ë§¥ìŠ¤ì—”ì˜\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2804 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 13\n",
      "7. ì„¸ì • íƒ€ì„ìŠ¤í€˜ì–´ì§€ì \n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2637 | í‰ì : 4 | ë¦¬ë·° ìˆ˜: 3\n",
      "8. ê¸ˆí™”ì£¼ë‹¨\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2628 | í‰ì : 3.8 | ë¦¬ë·° ìˆ˜: 4\n",
      "9. ë”ìŠˆíŠ¸í•˜ìš°ìŠ¤ ì˜ë“±í¬í™ˆí”ŒëŸ¬ìŠ¤\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2534 | í‰ì : 3.6 | ë¦¬ë·° ìˆ˜: 27\n",
      "10. ì²œì‚¬ë“¤ì˜í•©ì°½\n",
      "ë¹„ì„ í˜¸ ìœ ì‚¬ë„: 0.2291 | í‰ì : 4.3 | ë¦¬ë·° ìˆ˜: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# ì´ë¦„ê³¼ ë¦¬ë·°ë¥¼ í•¨ê»˜ ê³ ë ¤í•œ ë²¡í„° (ê°€ì¤‘ì¹˜ ì¡°ì ˆ ê°€ëŠ¥)\n",
    "def get_combined_place_vector(place, review_weight=1.0, name_weight=1.0):\n",
    "    review_vec = place.get(\"review_vector\", np.zeros(model.get_sentence_embedding_dimension()))\n",
    "    name_vec = get_sbert_embedding(place.get(\"name\", \"\"))\n",
    "    \n",
    "    if np.linalg.norm(review_vec) == 0 and np.linalg.norm(name_vec) == 0:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    total_weight = review_weight + name_weight\n",
    "    return (review_weight * review_vec + name_weight * name_vec) / total_weight\n",
    "\n",
    "# ì¥ì†Œë³„ ë¹„ì„ í˜¸ ìœ ì‚¬ë„ ê³„ì‚° (ì´ë¦„ í¬í•¨)\n",
    "for place in all_places:\n",
    "    combined_vec = get_combined_place_vector(place, review_weight=1.0, name_weight=1.0)\n",
    "    if np.linalg.norm(combined_vec) > 0 and np.linalg.norm(nonhope_mean_vector) > 0:\n",
    "        score = cosine_similarity([combined_vec], [nonhope_mean_vector])[0][0]\n",
    "        place[\"nonhope_score\"] = round(score, 4)\n",
    "    else:\n",
    "        place[\"nonhope_score\"] = 0.0\n",
    "\n",
    "# íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™” ë° ì¶œë ¥\n",
    "type_to_places = defaultdict(list)\n",
    "for place in all_places:\n",
    "    type_to_places[place[\"type\"]].append(place)\n",
    "\n",
    "print(\"\\në¹„ì„ í˜¸ í‚¤ì›Œë“œ ìœ ì‚¬ë„ (íƒ€ì…ë³„ ìƒìœ„ 3ê°œì”©):\")\n",
    "for place_type, places in type_to_places.items():\n",
    "    print(f\"\\nğŸ”¹ {place_type.title()}:\")\n",
    "    top_places = sorted(places, key=lambda x: x[\"nonhope_score\"], reverse=True)[:10]\n",
    "    for i, place in enumerate(top_places, 1):\n",
    "        print(f\"{i}. {place['name']}\")\n",
    "        print(f\"ë¹„ì„ í˜¸ ìœ ì‚¬ë„: {place['nonhope_score']:.4f} | í‰ì : {place['rating']} | ë¦¬ë·° ìˆ˜: {place['user_ratings_total']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43834683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_place_for_json(place):\n",
    "    p = place.copy()\n",
    "    for key in [\"review_vector\", \"name_vector\"]:\n",
    "        if isinstance(p.get(key), np.ndarray):\n",
    "            p[key] = p[key].tolist()\n",
    "    if \"cluster_scores\" in p:\n",
    "        p[\"cluster_scores\"] = list(map(float, p[\"cluster_scores\"]))\n",
    "    if \"nonhope_score\" in p:\n",
    "        p[\"nonhope_score\"] = float(p[\"nonhope_score\"])\n",
    "    return p\n",
    "\n",
    "json_ready = [convert_place_for_json(p) for p in all_places]\n",
    "\n",
    "with open(\"all_places_embedding.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_ready, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3521109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"all_places_embedding.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_places = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time, datetime, timedelta\n",
    "\n",
    "# ë¶„ë‹¨ìœ„ ì‹œê°„ ì°¨ì´ ê³„ì‚° í•¨ìˆ˜\n",
    "def time_diff_minutes(t1, t2):\n",
    "    dt1 = datetime.combine(datetime.today(), t1)\n",
    "    dt2 = datetime.combine(datetime.today(), t2)\n",
    "    return abs((dt1 - dt2).total_seconds() / 60)\n",
    "\n",
    "time_table = []\n",
    "\n",
    "class ScheduleItem:\n",
    "    def __init__(self, title, start, end, place_type, location_info=None):\n",
    "        self.title = title\n",
    "        self.start = start  \n",
    "        self.end = end     \n",
    "        self.place_type = place_type\n",
    "        self.location_info = location_info\n",
    "        \n",
    "\n",
    "def generate_empty_slots(time_table, day_start=time(9, 0), day_end=time(23, 59)):\n",
    "    empty_slots = []\n",
    "\n",
    "    def to_datetime(t):\n",
    "        return datetime.combine(datetime.today(), t)\n",
    "\n",
    "    # ì •ë ¬ëœ íƒ€ì„í…Œì´ë¸”ë¡œ ê°€ì •\n",
    "    sorted_table = sorted(time_table, key=lambda x: x.start)\n",
    "\n",
    "    # Step 1. ì²˜ìŒ ~ ì²« ì¼ì • ì „ êµ¬ê°„\n",
    "    if not sorted_table or sorted_table[0].start > day_start:\n",
    "        empty_slots += split_empty_range(day_start, sorted_table[0].start if sorted_table else day_end)\n",
    "\n",
    "    # Step 2. ì¼ì • ì‚¬ì´ ë¹ˆ ê³µê°„ ì°¾ê¸°\n",
    "    for i in range(len(sorted_table) - 1):\n",
    "        current_end = sorted_table[i].end\n",
    "        next_start = sorted_table[i + 1].start\n",
    "        if current_end < next_start:\n",
    "            empty_slots += split_empty_range(current_end, next_start)\n",
    "\n",
    "    # Step 3. ë§ˆì§€ë§‰ ì¼ì • ~ í•˜ë£¨ ë\n",
    "    if sorted_table and sorted_table[-1].end < day_end:\n",
    "        empty_slots += split_empty_range(sorted_table[-1].end, day_end)\n",
    "\n",
    "    return empty_slots\n",
    "\n",
    "def split_empty_range(start_time, end_time):\n",
    "    slots = []\n",
    "    dt_start = datetime.combine(datetime.today(), start_time)\n",
    "    dt_end = datetime.combine(datetime.today(), end_time)\n",
    "    gap_minutes = int((dt_end - dt_start).total_seconds() // 60)\n",
    "\n",
    "    if gap_minutes < 90:\n",
    "        return []  # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œ\n",
    "\n",
    "    elif gap_minutes < 120:\n",
    "        # 1ì‹œê°„ 30ë¶„ ì´ìƒ 2ì‹œê°„ ë¯¸ë§Œ â†’ í•˜ë‚˜ì˜ ìŠ¬ë¡¯\n",
    "        slots.append(ScheduleItem(None, start_time, end_time, None))\n",
    "    else:\n",
    "        # 2ì‹œê°„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°\n",
    "        dt_cursor = dt_start\n",
    "        while (dt_end - dt_cursor).total_seconds() >= 120 * 60:\n",
    "            dt_next = dt_cursor + timedelta(minutes=120)\n",
    "            slots.append(ScheduleItem(None, dt_cursor.time(), dt_next.time(), None))\n",
    "            dt_cursor = dt_next\n",
    "\n",
    "        # ë‚¨ì€ ì‹œê°„ ì²˜ë¦¬\n",
    "        remaining_minutes = int((dt_end - dt_cursor).total_seconds() // 60)\n",
    "        if 120 <= remaining_minutes <= 210:  # 2ì‹œê°„ ì´ìƒ 3ì‹œê°„ 30ë¶„ ì´í•˜ë©´ í•˜ë‚˜ë¡œ ë¬¶ê¸°\n",
    "            slots.append(ScheduleItem(None, dt_cursor.time(), dt_end.time(), None))\n",
    "        elif remaining_minutes >= 90:\n",
    "            # 1ì‹œê°„ 30ë¶„ ì´ìƒì´ë©´ ë§ˆì§€ë§‰ ìŠ¬ë¡¯ìœ¼ë¡œë„ ì¸ì •\n",
    "            slots.append(ScheduleItem(None, dt_cursor.time(), dt_end.time(), None))\n",
    "\n",
    "    return slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0f1d7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 (Friday) | From ì‹ ë„ë¦¼ì—­ to ì‹ ë„ë¦¼ ìˆ™ì†Œ\n",
      "  09:00 - 10:00 | ì‹ ë„ë¦¼ì—­\n",
      "  10:00 - 12:00 | None\n",
      "  12:00 - 14:00 | None\n",
      "  14:00 - 16:00 | None\n",
      "  16:00 - 18:00 | None\n",
      "  18:00 - 20:00 | None\n",
      "  20:00 - 22:00 | None\n",
      "  22:00 - 23:00 | ë¼ë§ˆë‹¤ ì„œìš¸ ì‹ ë„ë¦¼ í˜¸í…”\n",
      "2025-08-02 (Saturday) | From ì‹ ë„ë¦¼ ìˆ™ì†Œ to ì‹ ë„ë¦¼ ìˆ™ì†Œ\n",
      "  08:00 - 09:00 | ë¼ë§ˆë‹¤ ì„œìš¸ ì‹ ë„ë¦¼ í˜¸í…”\n",
      "  09:00 - 11:00 | None\n",
      "  11:00 - 13:00 | None\n",
      "  13:00 - 15:00 | None\n",
      "  15:00 - 17:00 | None\n",
      "  17:00 - 19:00 | None\n",
      "  19:00 - 21:00 | None\n",
      "  21:00 - 23:00 | None\n",
      "  23:00 - 00:00 | ë¼ë§ˆë‹¤ ì„œìš¸ ì‹ ë„ë¦¼ í˜¸í…”\n",
      "2025-08-03 (Sunday) | From ì‹ ë„ë¦¼ ìˆ™ì†Œ to ì‹ ë„ë¦¼ì—­\n",
      "  08:00 - 09:00 | ë¼ë§ˆë‹¤ ì„œìš¸ ì‹ ë„ë¦¼ í˜¸í…”\n",
      "  09:00 - 11:00 | None\n",
      "  11:00 - 13:00 | None\n",
      "  13:00 - 15:00 | None\n",
      "  15:00 - 17:00 | None\n",
      "  17:00 - 19:00 | None\n",
      "  19:00 - 21:00 | None\n",
      "  21:00 - 22:00 | ì‹ ë„ë¦¼ì—­\n"
     ]
    }
   ],
   "source": [
    "def place_location_info(place_name, api_key):\n",
    "    import requests\n",
    "\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "    params = {\n",
    "        \"query\": place_name,\n",
    "        \"key\": api_key,\n",
    "        \"language\": \"ko\"\n",
    "    }\n",
    "\n",
    "    res = requests.get(url, params=params).json()\n",
    "    if not res.get(\"results\"):\n",
    "        return None\n",
    "\n",
    "    top_result = res[\"results\"][0]\n",
    "    return {\n",
    "        \"name\": top_result.get(\"name\"),\n",
    "        \"lat\": top_result.get(\"geometry\", {}).get(\"location\", {}).get(\"lat\"),\n",
    "        \"lng\": top_result.get(\"geometry\", {}).get(\"location\", {}).get(\"lng\")\n",
    "    }\n",
    "\n",
    "def create_empty_daily_tables(API_KEY, start_date_str, end_date_str, \n",
    "                              first_day_start_time, last_day_end_time, \n",
    "                              start_location, final_end_location,\n",
    "                              accommodation_location,\n",
    "                              default_start_time=time(9, 0), default_end_time=time(23, 0)):\n",
    "    \"\"\"\n",
    "    ì—¬í–‰ ì‹œì‘~ì¢…ë£Œ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ, í˜„ì‹¤ì ì¸ ì¡°ê±´ ë°˜ì˜:\n",
    "    - ì²«ë‚ ë§Œ ì‚¬ìš©ì ì‹œì‘ì‹œê°„/ì¥ì†Œ\n",
    "    - ë§ˆì§€ë§‰ ë‚ ë§Œ ì‚¬ìš©ì ì¢…ë£Œì‹œê°„/ì¥ì†Œ\n",
    "    - ë‚˜ë¨¸ì§€ëŠ” ìˆ™ì†Œì—ì„œ ì‹œì‘/ì¢…ë£Œ\n",
    "    \"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "    num_days = (end_date - start_date).days + 1\n",
    "\n",
    "    daily_tables = {}\n",
    "    table_place_info = {}\n",
    "\n",
    "    for i in range(num_days):\n",
    "        date = start_date + timedelta(days=i)\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "        weekday = date.strftime(\"%A\")\n",
    "\n",
    "        is_first_day = i == 0\n",
    "        is_last_day = i == (num_days - 1)\n",
    "\n",
    "        # â° ì‹œê°„ ì„¤ì •\n",
    "        start_time = first_day_start_time if is_first_day else default_start_time\n",
    "        end_time = last_day_end_time if is_last_day else default_end_time\n",
    "\n",
    "        # ğŸ“ ìœ„ì¹˜ ì„¤ì •\n",
    "        start_loc = start_location if is_first_day else accommodation_location\n",
    "        end_loc = final_end_location if is_last_day else accommodation_location\n",
    "\n",
    "        # ë¹ˆ ìŠ¬ë¡¯ ìƒì„±\n",
    "        slots = split_empty_range(start_time, end_time)\n",
    "\n",
    "        daily_tables[date_str] = {\n",
    "            \"weekday\": weekday,\n",
    "            \"start_location\": start_loc,\n",
    "            \"end_location\": end_loc,\n",
    "            \"schedule\": slots\n",
    "        }\n",
    "        table_place_info[\"ì‹œì‘ìœ„ì¹˜\"] = place_location_info(start_location, API_KEY)\n",
    "        table_place_info[\"ì¢…ë£Œìœ„ì¹˜\"] = place_location_info(final_end_location, API_KEY)\n",
    "        table_place_info[\"ìˆ™ì†Œ\"] = place_location_info(accommodation_location, API_KEY)\n",
    "\n",
    "    return table_place_info, daily_tables\n",
    "\n",
    "API_KEY = \"AIzaSyBEl50H0xV7SnyNwcc0Yo-Ru-iiTXTBePc\"\n",
    "\n",
    "table_place_info, tables = create_empty_daily_tables(\n",
    "    API_KEY,\n",
    "    start_date_str=\"2025-08-01\",\n",
    "    end_date_str=\"2025-08-03\",\n",
    "    first_day_start_time=time(10, 0),\n",
    "    last_day_end_time=time(21, 0),\n",
    "    start_location=\"ì‹ ë„ë¦¼ì—­\",\n",
    "    final_end_location=\"ì‹ ë„ë¦¼ì—­\",\n",
    "    accommodation_location=\"ì‹ ë„ë¦¼ ìˆ™ì†Œ\"\n",
    ")\n",
    "\n",
    "def insert_initial_schedule_items_dynamic(daily_tables, table_place_info):\n",
    "    \"\"\"\n",
    "    ì¼ì • í…Œì´ë¸”ì— ì‹œì‘ ì „/ì¢…ë£Œ í›„ ì„ì˜ ì¼ì • ì‚½ì…\n",
    "    - ì‹œì‘ ì „: start_time ê¸°ì¤€, 1ì‹œê°„ ì „ ì¼ì •\n",
    "    - ì¢…ë£Œ í›„: end_time ê¸°ì¤€, 1ì‹œê°„ í›„ ì¼ì •\n",
    "    \"\"\"\n",
    "    for idx, (date, info) in enumerate(daily_tables.items()):\n",
    "        schedule = info[\"schedule\"]\n",
    "        start_time = schedule[0].start if schedule else time(9, 0)\n",
    "        end_time = schedule[-1].end if schedule else time(21, 0)\n",
    "\n",
    "        items_to_insert = []\n",
    "\n",
    "        # ì‹œì‘ ì „ ì¼ì •\n",
    "        if idx == 0:\n",
    "            # ì²«ë‚  â†’ ì¶œë°œì§€\n",
    "            title = table_place_info[\"ì‹œì‘ìœ„ì¹˜\"][\"name\"]\n",
    "            loc_info = table_place_info[\"ì‹œì‘ìœ„ì¹˜\"]\n",
    "            new_start = (datetime.combine(datetime.today(), start_time) - timedelta(hours=1)).time()\n",
    "            items_to_insert.append(ScheduleItem(title, new_start, start_time, \"start\", loc_info))\n",
    "        else:\n",
    "            # ì¤‘ê°„ë‚  or ë§ˆì§€ë§‰ë‚  â†’ ìˆ™ì†Œ\n",
    "            title = table_place_info[\"ìˆ™ì†Œ\"][\"name\"]\n",
    "            loc_info = table_place_info[\"ìˆ™ì†Œ\"]\n",
    "            new_start = (datetime.combine(datetime.today(), start_time) - timedelta(hours=1)).time()\n",
    "            items_to_insert.append(ScheduleItem(title, new_start, start_time, \"accommodation\", loc_info))\n",
    "\n",
    "        # ì¢…ë£Œ í›„ ì¼ì •\n",
    "        if idx == 0 and len(daily_tables)!=0:\n",
    "            # ì²«ë‚  â†’ ë„ì°©ì§€ (í•˜ë£¨ ë§ˆë¬´ë¦¬ìš©)\n",
    "            title = table_place_info[\"ìˆ™ì†Œ\"][\"name\"]\n",
    "            loc_info = table_place_info[\"ìˆ™ì†Œ\"]\n",
    "        elif idx == len(daily_tables) - 1:\n",
    "            # ë§ˆì§€ë§‰ë‚  â†’ ì¢…ë£Œì§€ì \n",
    "            title = table_place_info[\"ì¢…ë£Œìœ„ì¹˜\"][\"name\"]\n",
    "            loc_info = table_place_info[\"ì¢…ë£Œìœ„ì¹˜\"]\n",
    "        else:\n",
    "            # ì¤‘ê°„ë‚  â†’ ìˆ™ì†Œ\n",
    "            title = table_place_info[\"ìˆ™ì†Œ\"][\"name\"]\n",
    "            loc_info = table_place_info[\"ìˆ™ì†Œ\"]\n",
    "\n",
    "        new_end = (datetime.combine(datetime.today(), end_time) + timedelta(hours=1)).time()\n",
    "        items_to_insert.append(ScheduleItem(title, end_time, new_end, \"end\" if idx == len(daily_tables) - 1 else \"accommodation\", loc_info))\n",
    "\n",
    "        # ì‚½ì… (ì•, ë’¤ ìˆœì„œ ë³´ì¥)\n",
    "        schedule.insert(0, items_to_insert[0])\n",
    "        schedule.append(items_to_insert[1])\n",
    "\n",
    "    return daily_tables\n",
    "\n",
    "tables = insert_initial_schedule_items_dynamic(tables, table_place_info)\n",
    "\n",
    "for date, data in tables.items():\n",
    "    print(f\"{date} ({data['weekday']}) | From {data['start_location']} to {data['end_location']}\")\n",
    "    for item in data['schedule']:\n",
    "        print(f\"  {item.start.strftime('%H:%M')} - {item.end.strftime('%H:%M')} | {item.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª…ì†Œ ì œì•½ ì¡°ê±´ ëŒ€ì…\n",
    "def get_constraints(base_mode=\"ëª…ì†Œ ì¤‘ì‹¬\"):\n",
    "    constraints = {\n",
    "        \"must_visit_attraction_every_minutes\": 180,  # 3ì‹œê°„ : ë§ˆì§€ë§‰ ëª…ì†Œ ë°©ë¬¸ í›„ ì¼ì • ì‹œê°„ì´ ì§€ë‚˜ë©´ ë°˜ë“œì‹œ ìƒˆë¡œìš´ ëª…ì†Œë¥¼ ì¶”ì²œí•´ì•¼ í•œë‹¤ëŠ” ì œì•½\n",
    "        \"attraction_required\": True, # í•˜ë£¨ ì¼ì •ì— ëª…ì†Œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ì—¬ë¶€\n",
    "\n",
    "        \"min_minutes_between_meals\": 360,  # 6ì‹œê°„ : ì‹ì‚¬ í›„ ì‹ì‚¬ë¥¼ ë°˜ë“œì‹œ í•´ì•¼í•˜ëŠ” ì œì•½ ì—¬ë¶€\n",
    "        \"require_meal_after_threshold\": True, # ì‹ì‚¬ë¥¼ í•˜ì§€ ì•Šê³  ì¼ì • ì‹œê°„ì´ ì§€ë‚˜ë©´, ë‹¤ìŒ ì¼ì •ìœ¼ë¡œ ë°˜ë“œì‹œ ì‹ì‚¬ë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤ëŠ” ì¡°ê±´\n",
    "        \"dont_eat_meal\": 180, #ì‹ì‚¬ í›„ ì§€ë‚˜ê°€ì•¼í•˜ëŠ” ì‹œê°„ ì—¬ë¶€\n",
    "\n",
    "        \"department_store_required_interval\": None, # ì¼ì •ì´ íŠ¹ì • ì‹œê°„(ë¶„ ë‹¨ìœ„) ì´ìƒ ì§€ë‚  ë•Œë§ˆë‹¤ ë°±í™”ì ì´ë‚˜ ì‡¼í•‘ëª°ì„ ê¼­ ì¼ì •ì— ë„£ì–´ì•¼ í•œë‹¤ëŠ” ì œì•½ ì¡°ê±´\n",
    "\n",
    "        \"allow_multiple_cafes\": False # í•˜ë£¨ ì¼ì •ì— ì¹´í˜(ë˜ëŠ” ìœ ì‚¬ ì¥ì†Œ: ë¹µì§‘ ë“±)ë¥¼ ì—°ì†ìœ¼ë¡œ í¬í•¨í•˜ëŠ” ê²ƒì„ í—ˆìš©í• ì§€ ì—¬ë¶€\n",
    "    }\n",
    "    # ì¶”ê°€ ì„ íƒ ì˜µì…˜ ë°˜ì˜\n",
    "    if base_mode ==\"ì‹ì‚¬ ì¤‘ì‹¬\": # ëª…ì†Œ í•„ìˆ˜ ì œê±°í•˜ê³  ì‹ì‚¬ ê°€ê²Œ ì œì•½ì„ 3ì‹œê°„ìœ¼ë¡œ ê°ì†Œ\n",
    "        constraints[\"attraction_required\"] = False\n",
    "        constraints[\"min_minutes_between_meals\"] = 180\n",
    "\n",
    "    if base_mode ==\"ì¹´í˜, ë¹µì§‘ ì¤‘ì‹¬\": # ì‹ì‚¬ í•„ìˆ˜ ì¡°ê±´ ì œê±°í•˜ê³  ëª…ì†Œ í•„ìˆ˜ ì œê±°, ì¹´í˜ì—°ì† í—ˆìš©\n",
    "        constraints[\"require_meal_after_threshold\"] = False\n",
    "        constraints[\"attraction_required\"] = False\n",
    "        constraints[\"allow_multiple_cafes\"] = True\n",
    "\n",
    "    if base_mode ==\"ì‡¼í•‘ ì¤‘ì‹¬\": # ëª…ì†Œì¡°ê±´ ì œê±° ë° ë°±í™”ì  ê°œìˆ˜ ì œí•œ í•´ì œ\n",
    "        constraints[\"department_store_required_interval\"] = 180  # 3ì‹œê°„ë§ˆë‹¤ ì‡¼í•‘\n",
    "        constraints[\"attraction_required\"] = False\n",
    "\n",
    "    return constraints\n",
    "\n",
    "# ì¸ë±ìŠ¤ ê¸°ì¤€ ì§€ë‚œ ìµœëŒ€ ì‹œê°„ ë°˜í™˜\n",
    "def get_elapsed_minutes_since_last_type(place_type, time_table, idx):\n",
    "    \"\"\"í˜„ì¬ ì¸ë±ìŠ¤ ê¸°ì¤€, í•´ë‹¹ íƒ€ì…(place_type)ì˜ ë§ˆì§€ë§‰ ë°©ë¬¸ìœ¼ë¡œë¶€í„° ê²½ê³¼í•œ ì‹œê°„(ë¶„)ì„ ë°˜í™˜\"\"\"\n",
    "    current_start = time_table[idx].start\n",
    "\n",
    "    for i in range(idx - 1, -1, -1):  # í˜„ì¬ index ì´ì „ë§Œ íƒìƒ‰\n",
    "        if time_table[i].place_type == place_type:\n",
    "            last_end = time_table[i].end\n",
    "            return time_diff_minutes(last_end, current_start)\n",
    "\n",
    "    return None  # í•´ë‹¹ íƒ€ì…ì´ ì•„ì˜ˆ ì—†ì—ˆë˜ ê²½ìš°\n",
    "\n",
    "# íƒ€ì… ì„ íƒ í•¨ìˆ˜\n",
    "def select_allowed_types(time_table, base_mode, idx):\n",
    "    allowed_types = ['tourist_attraction', 'cafe', 'restaurant', 'bakery', 'bar', 'shopping_mall']\n",
    "\n",
    "    constraints = get_constraints(base_mode)\n",
    "\n",
    "    if constraints[\"attraction_required\"] == True:\n",
    "        if get_elapsed_minutes_since_last_type('tourist_attraction', time_table, idx)>=constraints[\"must_visit_attraction_every_minutes\"]:\n",
    "            allowed_types = ['tourist_attraction']\n",
    "            return allowed_types\n",
    "    \n",
    "    if constraints[\"require_meal_after_threshold\"] == True:\n",
    "        if get_elapsed_minutes_since_last_type('restaurant', time_table, idx)<=constraints[\"dont_eat_meal\"]:\n",
    "            allowed_types.remove(\"restaurant\")\n",
    "        if get_elapsed_minutes_since_last_type('restaurant', time_table, idx)>=constraints[\"min_minutes_between_meals\"]:\n",
    "            allowed_types = ['restaurant']\n",
    "            return allowed_types\n",
    "    \n",
    "    if constraints[\"department_store_required_interval\"] != None:\n",
    "        if get_elapsed_minutes_since_last_type('shopping_mall', time_table, idx)<=constraints[\"department_store_required_interval\"]:\n",
    "            allowed_types = ['shopping_mall']\n",
    "            return allowed_types\n",
    "    \n",
    "    if constraints[\"allow_multiple_cafes\"] == False:\n",
    "        if time_table[idx-1].place_type == \"cafe\" or time_table[idx-1].place_type == \"bakery\":\n",
    "            for t in [\"cafe\", \"bakery\"]:\n",
    "                allowed_types.remove(t)\n",
    "    \n",
    "    return allowed_types\n",
    "\n",
    "import math\n",
    "\n",
    "# ìœ í´ë¦¬ë“œ ê¸°ë°˜ ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜\n",
    "def euclidean(lat1, lon1, lat2, lon2):\n",
    "    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n",
    "\n",
    "# ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜\n",
    "def compute_distance_matrix(places): \n",
    "    n = len(places)\n",
    "    matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                matrix[i][j] = euclidean(\n",
    "                    places[i]['lat'], places[i]['lng'],\n",
    "                    places[j]['lat'], places[j]['lng']\n",
    "                )\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_place_open_during_slot(place, date_str, start_time, end_time):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ date_str (ì˜ˆ: '2025-08-01')ê³¼ time ê°ì²´ ê¸°ë°˜ìœ¼ë¡œ\n",
    "    í•´ë‹¹ ì¥ì†Œê°€ ì˜ì—…í•˜ëŠ”ì§€ í™•ì¸.\n",
    "    \"\"\"\n",
    "    if place.get(\"business_status\") != \"OPERATIONAL\":\n",
    "        return False\n",
    "\n",
    "    weekday = datetime.strptime(date_str, \"%Y-%m-%d\").weekday()\n",
    "    weekday_kr = [\"ì›”ìš”ì¼\", \"í™”ìš”ì¼\", \"ìˆ˜ìš”ì¼\", \"ëª©ìš”ì¼\", \"ê¸ˆìš”ì¼\", \"í† ìš”ì¼\", \"ì¼ìš”ì¼\"]\n",
    "    target_day = weekday_kr[weekday]\n",
    "\n",
    "    for text in place.get(\"weekday_text\", []):\n",
    "        if not text.startswith(target_day):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            hours = text.split(\": \", 1)[-1].split(\" ~ \")\n",
    "            open_dt = datetime.strptime(hours[0], \"ì˜¤ì „ %I:%M\") if \"ì˜¤ì „\" in hours[0] else datetime.strptime(hours[0], \"ì˜¤í›„ %I:%M\")\n",
    "            close_dt = datetime.strptime(hours[1], \"ì˜¤ì „ %I:%M\") if \"ì˜¤ì „\" in hours[1] else datetime.strptime(hours[1], \"ì˜¤í›„ %I:%M\")\n",
    "            open_time = open_dt.time()\n",
    "            close_time = close_dt.time()\n",
    "\n",
    "            if open_time <= start_time and end_time <= close_time:\n",
    "                return True\n",
    "        except:\n",
    "            continue  # í¬ë§· ì´ìƒí•œê±´ ê·¸ëƒ¥ ë„˜ê¹€\n",
    "\n",
    "    return False\n",
    "\n",
    "def get_valid_candidates(all_places, allowed_types, date_str, slot):\n",
    "    \"\"\"\n",
    "    - allowed_typesì— í•´ë‹¹í•˜ëŠ” ì¥ì†Œ ì¤‘\n",
    "    - slot ì‹œê°„ëŒ€ì— ì—´ë ¤ìˆëŠ” ê³³ë§Œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    for place in all_places:\n",
    "        if place['type'] not in allowed_types:\n",
    "            continue\n",
    "        if is_place_open_during_slot(place, date_str, slot.start, slot.end):\n",
    "            candidates.append(place)\n",
    "\n",
    "    return candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
